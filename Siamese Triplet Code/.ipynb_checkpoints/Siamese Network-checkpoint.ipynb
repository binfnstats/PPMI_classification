{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/michaelallwright/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'random'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-b0c6576765ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tf.random.normal(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'random'"
     ]
    }
   ],
   "source": [
    "tf.random.normal(\n",
    "    shape, mean=0.0, stddev=1.0, dtype=tf.dtypes.float32, seed=None, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdbp=\"/Users/michaelallwright/Documents/python/PDBP/Project Final/data/\"\n",
    "mod_dataPDBP=pd.read_csv('%s%s' % (pdbp,'mod_dataPDBP.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 14322)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_dataPDBP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=mod_dataPDBP.iloc[:, : 12500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'x_width': 50,\n",
    "    'x_height': 50,\n",
    "    'num_channels': 5,\n",
    "    'dropout': [0.1, 0.1],\n",
    "    'embedding_size': 256,\n",
    "    'batch_size': 256,\n",
    "    'positives_per_batch_count': 40,\n",
    "    'triplets_count': 1024,\n",
    "    'masked_count': 4,\n",
    "    'alpha_margin': 0.19,\n",
    "    'learning_rate': 2e-5,\n",
    "\n",
    "    'tfrecords_train': './TF_Records_File_train.tfrecords',\n",
    "    'tfrecords_cv': './TF_Records_File_cv.tfrecords',\n",
    "\n",
    "    'model_path': './trained_model.ckpt',\n",
    "    'log_path': './train_log.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #assume you data is in your dataframe\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    row_tensor = row.reshape([config['num_channels'],\n",
    "                               config['x_height'],\n",
    "                               config['x_width']])\n",
    "    batch_data = np.zeros([config['batch_size'],\n",
    "                               config['num_channels'],\n",
    "                               config['x_height'],\n",
    "                               config['x_width']], dtype = np.float32)\n",
    "    batch_data[0, :, :, :] = row_tensor \n",
    "    rows_positive_tensor = get_positives(row, df, config['positives_per_batch_count'])\n",
    "    batch_data[1:(config['positives_per_batch_count'] + 1), :, :, :] = rows_positive_tensor\n",
    "    batch_data[(config['positives_per_batch_count'] + 1):, :, :, :] = rows_negative_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapedata(df):\n",
    "    df=df.drop(['sample','Diagnosis'],axis=1)\n",
    "    for i in range(df.shape[0]):\n",
    "        x1=np.asarray(df.iloc[i:i+1, : 12500])\n",
    "        x1=x1.reshape(1,5,50,50,)\n",
    "        if i==0:\n",
    "            x_full=x1\n",
    "        else:\n",
    "            x_full=np.vstack((x_full,x1))\n",
    "    return x_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdpb_rs_pd=reshapedata(mod_dataPDBP[(mod_dataPDBP['Diagnosis']==\"PD\")])\n",
    "pdpb_rs_hc=reshapedata(mod_dataPDBP[(mod_dataPDBP['Diagnosis']==\"HC\")])\n",
    "#repeat the negatives to fill batch size\n",
    "pdpb_rs_pd=np.vstack((pdpb_rs_pd,pdpb_rs_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 5, 50, 50)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pdpb_rs_hc[1:(config['positives_per_batch_count'] + 1),:,:,:].shape\n",
    "#pdpb_rs_pd[1:config['batch_size']-config['positives_per_batch_count'],:,:,:].shape\n",
    "#pdpb_rs_pd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_channels=5\n",
    "height=50\n",
    "width-50\n",
    "\n",
    "positives per batch count?\n",
    "How to define anchor, positive and negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator():\n",
    "    # each batch contains an anchor x at index 0 followed by\n",
    "    # config[\"positives_per_batch_count\"] positives followed by\n",
    "    # negatives.\n",
    "    \n",
    "     ### ==> TODO: You need to write this function for your case to use this code\n",
    "    train_ratio = .7\n",
    "    iters = 1e5\n",
    "\n",
    "    for _iter in range(iters):\n",
    "        batch_data = np.zeros([config['batch_size'],\n",
    "                               config['num_channels'],\n",
    "                               config['x_height'],\n",
    "                               config['x_width']], dtype = np.float32)\n",
    "\n",
    "        batch_data[0, :, :, :] = pdpb_rs_hc[0,:,:,:]#anchor_data        \n",
    "        batch_data[1:(config['positives_per_batch_count'] + 1), :, :, :] = pdpb_rs_hc[1:(config['positives_per_batch_count'] + 1),:,:,:]#positive_data\n",
    "        batch_data[(config['positives_per_batch_count'] + 1):, :, :, :] = pdpb_rs_pd[1:config['batch_size']-config['positives_per_batch_count'],:,:,:]#negative_data\n",
    "        \n",
    "        if i <= np.int(train_ratio * iters):\n",
    "            batch_type = 'train'\n",
    "        else:\n",
    "            batch_type = 'cv'\n",
    "        yield np.array(batch_data).tobytes(), batch_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ratio = .7\n",
    "iters = 1e5\n",
    "np.int(train_ratio * iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    " def write_to_tfrecord():\n",
    "    # write batches to tfrecords file\n",
    "    data_generator = batch_generator()\n",
    "    writer_train = tf.python_io.TFRecordWriter(config['tfrecords_train'])\n",
    "    writer_cv = tf.python_io.TFRecordWriter(config['tfrecords_cv'])\n",
    "    train_counter = 0\n",
    "    cv_counter = 0\n",
    "    try:\n",
    "        while(1):\n",
    "            batch, batch_type = next(data_generator)\n",
    "            feature = tf.train.Feature(bytes_list = tf.train.BytesList(value = [batch]))\n",
    "            feature_dict = {'batch':\n",
    "                tf.train.Feature(bytes_list = tf.train.BytesList(value = [batch]))}\n",
    "            example = tf.train.Example(features = tf.train.Features(feature = feature_dict))\n",
    "            if batch_type == 'train':\n",
    "                train_counter += 1\n",
    "                writer_train.write(example.SerializeToString())\n",
    "                print('>>>>>> train: ', train_counter, end = '\\r')\n",
    "            else:\n",
    "                cv_counter += 1\n",
    "                writer_cv.write(example.SerializeToString())\n",
    "                print('>>>>>> cv: ', cv_counter, end = '\\r')\n",
    "    except:\n",
    "        print('')\n",
    "    finally:\n",
    "        print('total_number of trained records:', train_counter)\n",
    "        print('total_number of cv records:', cv_counter)\n",
    "        writer_train.close()\n",
    "        writer_cv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_tfrecord(tfrecord_file, queue_size, num_threads, min_capacity):\n",
    "    # read batch from tfrecords file\n",
    "    with tf.variable_scope('Queue_Batch_Shuffle', reuse = False) as scope:\n",
    "        tfrecord_file_queue = tf.train.string_input_producer(tfrecord_file, name = 'queue')\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, tfrecord_serialized = reader.read(tfrecord_file_queue)\n",
    "        tfrecord_features = tf.parse_single_example(tfrecord_serialized,\n",
    "                    features = {'batch': tf.FixedLenFeature([], tf.string)},\n",
    "                    name = 'features')\n",
    "\n",
    "        batch_data = tf.decode_raw(tfrecord_features['batch'], tf.float32)\n",
    "        batch_data = tf.reshape(batch_data, [config['batch_size'],\n",
    "                                             config['num_channels'],\n",
    "                                             config['x_height'],\n",
    "                                             config['x_width']])\n",
    "\n",
    "        batch_data_shuffled = tf.train.shuffle_batch([batch_data],\n",
    "                                                      batch_size = 1,\n",
    "                                                      capacity = queue_size,\n",
    "                                                      num_threads = num_threads,\n",
    "                                                      min_after_dequeue = min_capacity)\n",
    "        return batch_data_shuffled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_list_maker():\n",
    "    # make a list of <anchor_index, positive_index, negativeIndex> triplets\n",
    "    # of size config['triplets_count']\n",
    "    triplets = []\n",
    "    negatives_per_batch_count = config['batch_size'] - config['positives_per_batch_count']\n",
    "    positives_index = list(range(config['positives_per_batch_count']))\n",
    "    np.random.shuffle(positives_index)\n",
    "    for positive_idx in positives_index:\n",
    "        pos_idx = positive_idx + 1\n",
    "        negatives_index = list(range(negatives_per_batch_count))\n",
    "        np.random.shuffle(negatives_index)\n",
    "        for negative_idx in negatives_index:\n",
    "            neg_idx = negative_idx + 1 + config['positives_per_batch_count']\n",
    "            triplets.append([0, pos_idx, neg_idx])\n",
    "            if len(triplets) == config['triplets_count']:\n",
    "                result = np.array(triplets)\n",
    "                np.random.shuffle(result)\n",
    "                return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'random' has no attribute 'normal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-81d9071ee6eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m y = tf.keras.layers.Conv2D(\n\u001b[1;32m      5\u001b[0m 2, 3, activation='relu', input_shape=input_shape[1:])(x)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'random' has no attribute 'normal'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "input_shape = (4, 28, 28, 3)\n",
    "x = random.normal(input_shape)\n",
    "y = tf.keras.layers.Conv2D(\n",
    "2, 3, activation='relu', input_shape=input_shape[1:])(x)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x, dropout_rate = [0.1, 0.05], is_training = True, print_summary = False):\n",
    "    # build the CNN model\n",
    "    print('Build model...')\n",
    "    with tf.variable_scope('Model', reuse = False) as scope:\n",
    "        # block 1\n",
    "        conv1 = tf.layers.conv2d(x, 25, [3, 3],\n",
    "                                 strides = [1, 1],\n",
    "                                 data_format = 'channels_first',\n",
    "                                 padding = 'same',\n",
    "                                 name = 'conv1')\n",
    "        batch_norm1 = tf.layers.batch_normalization(conv1, training = is_training,\n",
    "                                                    axis = 1, name = 'batch_norm1')\n",
    "        relu1 = tf.nn.relu(batch_norm1, name = 'relu1')\n",
    "        max_pool1 = tf.layers.max_pooling2d(relu1, [2, 2],\n",
    "                                            strides = [2, 2],\n",
    "                                            data_format = 'channels_first',\n",
    "                                            padding = 'valid',\n",
    "                                            name = 'max_pool1')\n",
    "\n",
    "        # block 2\n",
    "        conv2 = tf.layers.conv2d(max_pool1, 50, [2, 2],\n",
    "                                 strides = [1, 1],\n",
    "                                 data_format = 'channels_first',\n",
    "                                 padding = 'same',\n",
    "                                 name = 'conv2')\n",
    "        batch_norm2 = tf.layers.batch_normalization(conv2, training = is_training,\n",
    "                                                    axis = 1, name = 'batch_norm2')\n",
    "        relu2 = tf.nn.relu(batch_norm2, name = 'relu2')\n",
    "        max_pool2 = tf.layers.max_pooling2d(relu2, [2, 2],\n",
    "                                            strides = [2, 2],\n",
    "                                            data_format = 'channels_first',\n",
    "                                            padding = 'valid',\n",
    "                                            name = 'max_pool2')\n",
    "\n",
    "        # block 3\n",
    "        conv3 = tf.layers.conv2d(max_pool2, 100, [3, 3],\n",
    "                                 strides = [1, 1],\n",
    "                                 data_format = 'channels_first',\n",
    "                                 padding = 'same',\n",
    "                                 name = 'conv3')\n",
    "        batch_norm3 = tf.layers.batch_normalization(conv3, training = is_training,\n",
    "                                                    axis = 1, name = 'batch_norm3')\n",
    "        relu3 = tf.nn.relu(batch_norm3, name = 'relu3')\n",
    "        max_pool3 = tf.layers.max_pooling2d(relu3, [2, 2],\n",
    "                                            strides = [2, 2],\n",
    "                                            data_format = 'channels_first',\n",
    "                                            padding = 'valid',\n",
    "                                            name = 'max_pool3')\n",
    "\n",
    "        # block 4\n",
    "        conv4 = tf.layers.conv2d(max_pool3, 100, [2, 2],\n",
    "                                 strides = [1, 1],\n",
    "                                 data_format = 'channels_first',\n",
    "                                 padding = 'same',\n",
    "                                 name = 'conv4')\n",
    "        batch_norm4 = tf.layers.batch_normalization(conv4, training = is_training,\n",
    "                                                    axis = 1, name = 'batch_norm4')\n",
    "        relu4 = tf.nn.relu(batch_norm4, name = 'relu4')\n",
    "        max_pool4 = tf.layers.max_pooling2d(relu4, [2, 2],\n",
    "                                            strides = [2, 2],\n",
    "                                            data_format = 'channels_first',\n",
    "                                            padding = 'valid',\n",
    "                                            name = 'max_pool4')\n",
    "        dropout4 = tf.layers.dropout(max_pool4, dropout_rate[0], name = 'dropout4')\n",
    "\n",
    "        # block 5\n",
    "        flatten_length = dropout4.get_shape().as_list()[1] * \\\n",
    "                         dropout4.get_shape().as_list()[2] * \\\n",
    "                         dropout4.get_shape().as_list()[3]\n",
    "        flatten5 = tf.reshape(dropout4, (-1, flatten_length), name = 'flatten5')\n",
    "        fc5 = tf.layers.dense(flatten5, 1024, name = 'fc5')\n",
    "        batch_norm5 = tf.layers.batch_normalization(fc5, training = is_training,\n",
    "                                                    name = 'batch_norm5')\n",
    "        relu5 = tf.nn.relu(batch_norm5, name = 'relu5')\n",
    "\n",
    "        # block 6\n",
    "        fc6 = tf.layers.dense(relu5, 1024, name = 'fc6')\n",
    "        batch_norm6 = tf.layers.batch_normalization(fc6, training = is_training,\n",
    "                                                    name = 'batch_norm6')\n",
    "        relu6 = tf.nn.relu(batch_norm6, name = 'relu6')\n",
    "\n",
    "        # block 7\n",
    "        fc7 = tf.layers.dense(relu6, config['embedding_size'], name = 'fc7')\n",
    "        batch_norm7 = tf.layers.batch_normalization(fc7, training = is_training,\n",
    "                                                    name = 'batch_norm7')\n",
    "        relu7 = tf.nn.relu(batch_norm7, name = 'relu7')\n",
    "        dropout7 = tf.layers.dropout(relu7, dropout_rate[1], name = 'dropout7')\n",
    "        l27 = tf.nn.l2_normalize(fc7, 1, name ='l27')\n",
    "\n",
    "        # block 8\n",
    "        fc8 = tf.layers.dense(dropout7, config['embedding_size'], name = 'fc8')\n",
    "        l28 = tf.nn.l2_normalize(fc8, 1, name ='l28')\n",
    "\n",
    "        assert fc8.get_shape()[1] == config['embedding_size']\n",
    "        if print_summary:\n",
    "            print('Model summary:\\n x: %s\\n' \\\n",
    "                  ' conv1: %s\\n max_pool1: %s\\n' \\\n",
    "                  ' conv2: %s\\n max_pool2: %s\\n' \\\n",
    "                  ' conv3: %s\\n max_pool3: %s\\n' \\\n",
    "                  ' conv4: %s\\n max_pool4 %s\\n' \\\n",
    "                  ' flatten5 %s\\n fc5 %s\\n' \\\n",
    "                  ' fc6 %s\\n fc7 %s\\n' \\\n",
    "                  ' fc8 %s\\n' %(x.get_shape(),\n",
    "                                conv1.get_shape(), max_pool1.get_shape(),\n",
    "                                conv2.get_shape(), max_pool2.get_shape(),\n",
    "                                conv3.get_shape(), max_pool3.get_shape(),\n",
    "                                conv4.get_shape(), max_pool4.get_shape(),\n",
    "                                flatten5.get_shape(), fc5.get_shape(),\n",
    "                                fc6.get_shape(), fc7.get_shape(), fc8.get_shape()))\n",
    "        return l27, l28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(loss):\n",
    "    # model optimizer\n",
    "     with tf.variable_scope('Optimizer', reuse = False) as scope:\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            all_vars = tf.trainable_variables()\n",
    "            model_vars = [var for var in all_vars if var.name.startswith('Model')]\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate = config['learning_rate']).minimize(loss, var_list = model_vars)\n",
    "            return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_triplet_embeddings(triplets, embeddings):\n",
    "    # prepare triplet embeddings\n",
    "    # triplets: (anchor_index, positive_index, negative_index)\n",
    "\n",
    "    with tf.variable_scope('Embeddings', reuse = False) as scope:\n",
    "        anchor_embeddings = tf.gather(embeddings, triplets[:, 0])\n",
    "        positive_embeddings = tf.gather(embeddings, triplets[:, 1])        \n",
    "        negative_embeddings = tf.gather(embeddings, triplets[:, 2])\n",
    "\n",
    "        return anchor_embeddings, positive_embeddings, negative_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss_acc(triplet_embeddings1, triplet_embeddings2, mask, threshold):\n",
    "    # triplet loss that is being minimized:\n",
    "    # loss = reduce_mean(l2_norm_squared(f(x_a), f(x_p)) -\n",
    "    #                    l2_norm_squared(f(x_a), f(x_n)) +\n",
    "    #                    alpha)\n",
    "\n",
    "    anchor_embeddings1, positive_embeddings1, nagative_embeddings1 = triplet_embeddings1\n",
    "    anchor_embeddings2, positive_embeddings2, nagative_embeddings2 = triplet_embeddings2\n",
    "\n",
    "    with tf.variable_scope('Loss', reuse = False) as scope:\n",
    "        ap_dist2 = tf.reduce_sum(tf.square(anchor_embeddings2 - positive_embeddings2), axis = -1)\n",
    "        an_dist2 = tf.reduce_sum(tf.square(anchor_embeddings2 - nagative_embeddings2), axis = -1)\n",
    "\n",
    "        flags = tf.cast(tf.greater(an_dist2, ap_dist2), tf.float32)\n",
    "        flags = tf.maximum(flags, mask) # let a few ap > an cases skip and be used in training (for exploration)\n",
    "        base_loss2  = (ap_dist2 - an_dist2 + config['alpha_margin']) * flags\n",
    "        loss2 = tf.reduce_sum(tf.maximum(base_loss2, 0))\n",
    "\n",
    "        ap_dist1 = tf.reduce_sum(tf.square(anchor_embeddings1 - positive_embeddings1), axis = -1)\n",
    "        an_dist1 = tf.reduce_sum(tf.square(anchor_embeddings1 - nagative_embeddings1), axis = -1)\n",
    "        base_loss1  = (ap_dist1 - an_dist1 + config['alpha_margin']) * flags\n",
    "        loss1 = tf.reduce_sum(tf.maximum(base_loss1, 0))\n",
    "\n",
    "        loss = loss1 + loss2\n",
    "\n",
    "        ap_acc = tf.reduce_mean(tf.cast(tf.greater(threshold, tf.sqrt(ap_dist2)), tf.float32))\n",
    "        an_acc = tf.reduce_mean(tf.cast(tf.greater(tf.sqrt(an_dist2), threshold), tf.float32))\n",
    "        triplets_used = tf.reduce_sum(flags)\n",
    "\n",
    "        return loss, loss2, ap_acc, an_acc, triplets_used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_count = 70000 # put your number here\n",
    "cv_batches_count = 30000 # put your number here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_on_imporvemnet(file_path, sess, cv_loss, cv_losses):\n",
    "  #  save model when there is improvemnet in cv_loss value\n",
    "    if cv_losses == [] or cv_loss < np.min(cv_losses):\n",
    "        # save the entire model\n",
    "        saver = tf.train.Saver(max_to_keep = 1)\n",
    "        saver.save(sess, file_path)\n",
    "        print('Model saved')\n",
    "\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(file_path, epoch, train_loss, cv_loss, log_mode = 'a'):\n",
    "    # log train and cv losses\n",
    "    mode = log_mode if epoch == 0 else 'a'\n",
    "\n",
    "    with open(file_path, mode) as f:\n",
    "        if mode == 'w':\n",
    "            header = 'epoch, train_loss, cv_loss\\n'\n",
    "            f.write(header)\n",
    "\n",
    "        line = '%d, %f, %f\\n' %(epoch, train_loss, cv_loss)\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cool_down_mask(epoch):\n",
    "    if epoch > 0 and epoch % 20 == 0:\n",
    "        if config['masked_count'] >= 1:\n",
    "            config['masked_count'] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrease_learning_rate(epoch):\n",
    "    #if epoch > 0 and epoch % 10 == 0:\n",
    "    #    if config['learning_rate'] > 0:\n",
    "    #        config['learning_rate'] = config['learning_rate'] * (1 - 1e-5)\n",
    "    pass # this function does nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mask(size, num_ones):\n",
    "    mask = np.zeros(size, dtype = np.float32)\n",
    "    index = list(range(size))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    for idx in index[:num_ones]:\n",
    "        mask[idx] = 1.0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_placeholders_tensors():\n",
    "    # get model's placeholders and tensors\n",
    "    x = tf.placeholder(tf.float32, name = 'x', shape = [None,\n",
    "                                                        config['num_channels'],\n",
    "                                                        config['x_height'],\n",
    "                                                        config['x_width']])\n",
    "    dropout_rate = tf.placeholder(tf.float32, name = 'dropout_rate', shape = [2])\n",
    "    is_training = tf.placeholder(tf.bool, name = 'is_training')\n",
    "    triplets = tf.placeholder(tf.int32, name = 'triplets', shape = [None, 3])\n",
    "    same_threshold = tf.placeholder(tf.float32, name = 'same_threshold')\n",
    "    mask = tf.placeholder(tf.float32, name='mask')\n",
    "    \n",
    "\n",
    "    embeddings1, embeddings2 = build_model(x, dropout_rate, is_training, print_summary = True)\n",
    "\n",
    "    triplet_embedings1 = prepare_triplet_embeddings(triplets, embeddings1)\n",
    "    triplet_embedings2 = prepare_triplet_embeddings(triplets, embeddings2)\n",
    "    loss, loss2, ap_acc, an_acc, triplets_used  = triplet_loss_acc(triplet_embedings1, triplet_embedings2, mask, same_threshold)\n",
    "    optim = optimizer(loss)\n",
    "\n",
    "    placeholders_tensors = {'x': x,\n",
    "                            'dropout_rate': dropout_rate,\n",
    "                            'is_training': is_training,\n",
    "                            'triplets': triplets,\n",
    "                            'embeddings1': embeddings1,\n",
    "                            'embeddings2': embeddings2,\n",
    "                            'same_threshold': same_threshold,\n",
    "                            'mask': mask,\n",
    "                            'optimizer': optim,\n",
    "                            'loss': loss2,\n",
    "                            'ap_acc': ap_acc,\n",
    "                            'an_acc': an_acc,\n",
    "                            'triplets_used': triplets_used}\n",
    "    return placeholders_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_per_batch(sess, batch, placeholders_tensors, epoch, threshold):\n",
    "    # train_per_batch and get train loss\n",
    "    tmp_loss, tmp_ap_acc, tmp_an_acc, tmp_triplets_used = [], [], [], []\n",
    "    t_total = 0\n",
    "    bad_batch_counter = 0\n",
    "\n",
    "    cool_down_mask(epoch)\n",
    "    decrease_learning_rate(epoch)\n",
    "    for iteration in range(train_batches_count):\n",
    "        t_start = time.time()\n",
    "        batch_x = sess.run(batch)\n",
    "        batch_triplets = triplet_list_maker()\n",
    "        train_mask = build_mask(config['triplets_count'], config['masked_count'])\n",
    "        feed_dictionary = {placeholders_tensors['x']: batch_x,\n",
    "                           placeholders_tensors['dropout_rate']: config['dropout'],\n",
    "                           placeholders_tensors['is_training']: True,\n",
    "                           placeholders_tensors['triplets']: batch_triplets,\n",
    "                           placeholders_tensors['same_threshold']: threshold,\n",
    "                           placeholders_tensors['mask']: train_mask}\n",
    "\n",
    "        sess.run(placeholders_tensors['optimizer'], feed_dict = feed_dictionary)\n",
    "        train_loss = sess.run(placeholders_tensors['loss'], feed_dict = feed_dictionary)\n",
    "        train_ap_acc = sess.run(placeholders_tensors['ap_acc'], feed_dict = feed_dictionary)\n",
    "        train_an_acc = sess.run(placeholders_tensors['an_acc'], feed_dict = feed_dictionary)\n",
    "        train_triplets_used = sess.run(placeholders_tensors['triplets_used'], feed_dict = feed_dictionary)  \n",
    "\n",
    "        tmp_loss.append(train_loss)\n",
    "        tmp_ap_acc.append(train_ap_acc)\n",
    "        tmp_an_acc.append(train_an_acc)\n",
    "        tmp_triplets_used.append(train_triplets_used)\n",
    "\n",
    "        if int(train_triplets_used) != config ['triplets_count']:\n",
    "            bad_batch_counter += 1\n",
    "        t_total += (time.time() - t_start)\n",
    "\n",
    "        print(' '*200, end = '\\r')\n",
    "        print('epoch: %d, time: %f | train_loss: %f |' \\\n",
    "              ' ap_acc: %f | an_acc: %f | triplets_used: %d | bads: %d' %(epoch, t_total, train_loss,\n",
    "                                                               train_ap_acc, train_an_acc,\n",
    "                                                               int(train_triplets_used), bad_batch_counter),\n",
    "                                                               end = '\\r')\n",
    "    train_loss = np.mean(tmp_loss)\n",
    "    train_ap_acc = np.mean(tmp_ap_acc)\n",
    "    train_an_acc = np.mean(tmp_an_acc)\n",
    "    train_triplets_used = np.mean(tmp_triplets_used)\n",
    "    print(' '*200, end = '\\r')\n",
    "    print('epoch: %d, time: %f | train_loss: %f |' \\\n",
    "          ' ap_acc: %f | an_acc: %f | triplets_used: %d | bads: %d\\n' %(epoch, t_total, train_loss,\n",
    "                                                                       train_ap_acc, train_an_acc,\n",
    "                                                                       int(train_triplets_used), bad_batch_counter),\n",
    "                                                                       end = '\\r')\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_per_batch(sess, batch, placeholders_tensors, epoch, threshold):\n",
    "    # cross-validate per batch and get cv loss and accuracy\n",
    "    tmp_loss, tmp_ap_acc, tmp_an_acc, tmp_triplets_used = [], [], [], []\n",
    "    t_total = 0\n",
    "    bad_batch_counter = 0\n",
    "\n",
    "    for iteration in range(cv_batches_count):\n",
    "        t_start = time.time()\n",
    "        batch_x = sess.run(batch)\n",
    "        batch_triplets = triplet_list_maker()\n",
    "        test_mask = build_mask(config['triplets_count'], config['masked_count'])\n",
    "        cv_feed_dictionary = {placeholders_tensors['x']: batch_x,\n",
    "                              placeholders_tensors['dropout_rate']: [0.0, 0.0],\n",
    "                              placeholders_tensors['is_training']: False,\n",
    "                              placeholders_tensors['triplets']: batch_triplets,\n",
    "                              placeholders_tensors['same_threshold']: threshold,\n",
    "                              placeholders_tensors['mask']: test_mask}\n",
    "        \n",
    "        cv_loss = sess.run(placeholders_tensors['loss'], feed_dict = cv_feed_dictionary)\n",
    "        cv_ap_acc = sess.run(placeholders_tensors['ap_acc'], feed_dict = cv_feed_dictionary)\n",
    "        cv_an_acc = sess.run(placeholders_tensors['an_acc'], feed_dict = cv_feed_dictionary)\n",
    "        cv_triplets_used = sess.run(placeholders_tensors['triplets_used'], feed_dict = cv_feed_dictionary)\n",
    "\n",
    "        tmp_loss.append(cv_loss)\n",
    "        tmp_ap_acc.append(cv_ap_acc)\n",
    "        tmp_an_acc.append(cv_an_acc)\n",
    "        tmp_triplets_used.append(cv_triplets_used)\n",
    "\n",
    "        if int(cv_triplets_used) != config['triplets_count']:\n",
    "            bad_batch_counter += 1\n",
    "        t_total += (time.time() - t_start)\n",
    "\n",
    "        print(' '*200, end = '\\r')\n",
    "        print('time: %f | cv_loss: %f |' \\\n",
    "              ' cv_ap_acc: %f | cv_an_acc: %f | cv_triplets_used: %d | cv_bads: %d' %(t_total, cv_loss,\n",
    "                                                                                      cv_ap_acc, cv_an_acc,\n",
    "                                                                                      int(cv_triplets_used),\n",
    "                                                                                      bad_batch_counter), end = '\\r')\n",
    "    cv_loss = np.mean(tmp_loss)\n",
    "    cv_ap_acc = np.mean(tmp_ap_acc)\n",
    "    cv_an_acc = np.mean(tmp_an_acc)\n",
    "    cv_triplets_used = np.mean(tmp_triplets_used)\n",
    "    print(' '*200, end = '\\r')\n",
    "    print('time: %f | cv_loss: %f |' \\\n",
    "          ' cv_ap_acc: %f | cv_an_acc: %f | cV_triplets_used: %d | cV_bads: %d' %(t_total, cv_loss,\n",
    "                                                                                  cv_ap_acc, cv_an_acc,\n",
    "                                                                                  int(cv_triplets_used),\n",
    "                                                                                  bad_batch_counter))\n",
    "    return cv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, init_epoch, queue_size, num_threads, resume, threshold):\n",
    "    # train model\n",
    "    train_losses, cv_losses = [], []\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    placeholders_tensors = get_placeholders_tensors()\n",
    "    train_batch  = read_from_tfrecord([config['tfrecords_train']], queue_size, num_threads,\n",
    "                                      np.int(0.5 * queue_size))\n",
    "    cv_batch  = read_from_tfrecord([config['tfrecords_cv']], 50, 2, 10)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess = sess, coord = coord)\n",
    "\n",
    "        if resume:\n",
    "            print('loading weights....')\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, (config['model_path']))  # to load the best saved model so far replace\n",
    "                                                              # \"curr_model_path\" with \"model_path\"\n",
    "\n",
    "            # load loss and accuracy so that less accurate model\n",
    "            # won't be saved after resume\n",
    "            tmp = np.genfromtxt(config['log_path'], delimiter = ',', names = True)\n",
    "            train_losses = list(tmp['train_loss'])\n",
    "            cv_losses = list(tmp['cv_loss'])\n",
    "            del tmp\n",
    "        else:\n",
    "            print('initializing weights....')\n",
    "            init_op =tf.group(tf.local_variables_initializer(), tf.global_variables_initializer())\n",
    "            sess.run(init_op)\n",
    "\n",
    "        print('training....')\n",
    "        for epoch in range(init_epoch, epochs):\n",
    "            # training\n",
    "            train_loss = train_per_batch(sess,train_batch, \n",
    "                                         placeholders_tensors, epoch, threshold)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            #cross-validation\n",
    "            cv_loss = cv_per_batch(sess, cv_batch, \n",
    "                                   placeholders_tensors, epoch, threshold)\n",
    "\n",
    "            #save model\n",
    "            save_model_on_imporvemnet(config['model_path'], sess, cv_loss, cv_losses)\n",
    "            cv_losses.append(cv_loss)\n",
    "\n",
    "            # log results\n",
    "            log_loss(config['log_path'], epoch, train_loss, cv_loss,\n",
    "                     log_mode = ('a' if resume else 'w'))\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        return train_losses, cv_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Queue_Batch_Shuffle_2/strided_slice:0' shape=(256, 5, 50, 50) dtype=float32>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_from_tfrecord([config['tfrecords_train']], 500, 10,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model summary:\n",
      " x: (?, 5, 50, 50)\n",
      " conv1: (?, 25, 50, 50)\n",
      " max_pool1: (?, 25, 25, 25)\n",
      " conv2: (?, 50, 25, 25)\n",
      " max_pool2: (?, 50, 12, 12)\n",
      " conv3: (?, 100, 12, 12)\n",
      " max_pool3: (?, 100, 6, 6)\n",
      " conv4: (?, 100, 6, 6)\n",
      " max_pool4 (?, 100, 3, 3)\n",
      " flatten5 (?, 900)\n",
      " fc5 (?, 1024)\n",
      " fc6 (?, 1024)\n",
      " fc7 (?, 256)\n",
      " fc8 (?, 256)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing weights....\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, ./TF_Records_File_train.tfrecords; No such file or directory\n",
      "\t [[Node: Queue_Batch_Shuffle/ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Queue_Batch_Shuffle/TFRecordReaderV2, Queue_Batch_Shuffle/queue)]]\n",
      "training....\n"
     ]
    },
    {
     "ename": "OutOfRangeError",
     "evalue": "RandomShuffleQueue '_8_Queue_Batch_Shuffle/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)\n\t [[Node: Queue_Batch_Shuffle/shuffle_batch = QueueDequeueManyV2[component_types=[DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Queue_Batch_Shuffle/shuffle_batch/random_shuffle_queue, Queue_Batch_Shuffle/shuffle_batch/n)]]\n\nCaused by op 'Queue_Batch_Shuffle/shuffle_batch', defined at:\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-72-0bd513eafeaa>\", line 3, in <module>\n    resume = False, threshold = 0.84)\n  File \"<ipython-input-69-43b5f473b446>\", line 8, in train_model\n    np.int(0.5 * queue_size))\n  File \"<ipython-input-51-486cedeffeb8>\", line 21, in read_from_tfrecord\n    min_after_dequeue = min_capacity)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py\", line 1300, in shuffle_batch\n    name=name)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py\", line 846, in _shuffle_batch\n    dequeued = queue.dequeue_many(batch_size, name=name)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 476, in dequeue_many\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 3480, in queue_dequeue_many_v2\n    component_types=component_types, timeout_ms=timeout_ms, name=name)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nOutOfRangeError (see above for traceback): RandomShuffleQueue '_8_Queue_Batch_Shuffle/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)\n\t [[Node: Queue_Batch_Shuffle/shuffle_batch = QueueDequeueManyV2[component_types=[DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Queue_Batch_Shuffle/shuffle_batch/random_shuffle_queue, Queue_Batch_Shuffle/shuffle_batch/n)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: RandomShuffleQueue '_8_Queue_Batch_Shuffle/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)\n\t [[Node: Queue_Batch_Shuffle/shuffle_batch = QueueDequeueManyV2[component_types=[DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Queue_Batch_Shuffle/shuffle_batch/random_shuffle_queue, Queue_Batch_Shuffle/shuffle_batch/n)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-0bd513eafeaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_model(epochs = 1000, init_epoch = 8,\n\u001b[1;32m      2\u001b[0m             \u001b[0mqueue_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m             resume = False, threshold = 0.84)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-43b5f473b446>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epochs, init_epoch, queue_size, num_threads, resume, threshold)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             train_loss = train_per_batch(sess,train_batch, \n\u001b[0;32m---> 36\u001b[0;31m                                          placeholders_tensors, epoch, threshold)\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-cd9015f2e806>\u001b[0m in \u001b[0;36mtrain_per_batch\u001b[0;34m(sess, batch, placeholders_tensors, epoch, threshold)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mt_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mbatch_triplets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriplet_list_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'triplets_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'masked_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: RandomShuffleQueue '_8_Queue_Batch_Shuffle/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)\n\t [[Node: Queue_Batch_Shuffle/shuffle_batch = QueueDequeueManyV2[component_types=[DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Queue_Batch_Shuffle/shuffle_batch/random_shuffle_queue, Queue_Batch_Shuffle/shuffle_batch/n)]]\n\nCaused by op 'Queue_Batch_Shuffle/shuffle_batch', defined at:\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-72-0bd513eafeaa>\", line 3, in <module>\n    resume = False, threshold = 0.84)\n  File \"<ipython-input-69-43b5f473b446>\", line 8, in train_model\n    np.int(0.5 * queue_size))\n  File \"<ipython-input-51-486cedeffeb8>\", line 21, in read_from_tfrecord\n    min_after_dequeue = min_capacity)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py\", line 1300, in shuffle_batch\n    name=name)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py\", line 846, in _shuffle_batch\n    dequeued = queue.dequeue_many(batch_size, name=name)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 476, in dequeue_many\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 3480, in queue_dequeue_many_v2\n    component_types=component_types, timeout_ms=timeout_ms, name=name)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/Users/michaelallwright/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nOutOfRangeError (see above for traceback): RandomShuffleQueue '_8_Queue_Batch_Shuffle/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)\n\t [[Node: Queue_Batch_Shuffle/shuffle_batch = QueueDequeueManyV2[component_types=[DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Queue_Batch_Shuffle/shuffle_batch/random_shuffle_queue, Queue_Batch_Shuffle/shuffle_batch/n)]]\n"
     ]
    }
   ],
   "source": [
    "train_model(epochs = 1000, init_epoch = 8,\n",
    "            queue_size = 500, num_threads = 10,\n",
    "            resume = False, threshold = 0.84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

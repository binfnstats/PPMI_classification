{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-12f3f6a9eac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m tf.random.normal(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shape' is not defined"
     ]
    }
   ],
   "source": [
    "tf.random.normal(\n",
    "    shape, mean=0.0, stddev=1.0, dtype=tf.dtypes.float32, seed=None, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdbp=\"/Users/michaelallwright/Documents/python/PDBP/Project Final/data/\"\n",
    "mod_dataPDBP=pd.read_csv('%s%s' % (pdbp,'mod_dataPDBP.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 14322)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_dataPDBP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=mod_dataPDBP.iloc[:, : 12500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'x_width': 50,\n",
    "    'x_height': 50,\n",
    "    'num_channels': 5,\n",
    "    'dropout': [0.1, 0.1],\n",
    "    'embedding_size': 256,\n",
    "    'batch_size': 256,\n",
    "    'positives_per_batch_count': 40,\n",
    "    'triplets_count': 1024,\n",
    "    'masked_count': 4,\n",
    "    'alpha_margin': 0.19,\n",
    "    'learning_rate': 2e-5,\n",
    "\n",
    "    'tfrecords_train': './TF_Records_File_train.tfrecords',\n",
    "    'tfrecords_cv': './TF_Records_File_cv.tfrecords',\n",
    "\n",
    "    'model_path': './trained_model.ckpt',\n",
    "    'log_path': './train_log.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #assume you data is in your dataframe\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    row_tensor = row.reshape([config['num_channels'],\n",
    "                               config['x_height'],\n",
    "                               config['x_width']])\n",
    "    batch_data = np.zeros([config['batch_size'],\n",
    "                               config['num_channels'],\n",
    "                               config['x_height'],\n",
    "                               config['x_width']], dtype = np.float32)\n",
    "    batch_data[0, :, :, :] = row_tensor \n",
    "    rows_positive_tensor = get_positives(row, df, config['positives_per_batch_count'])\n",
    "    batch_data[1:(config['positives_per_batch_count'] + 1), :, :, :] = rows_positive_tensor\n",
    "    batch_data[(config['positives_per_batch_count'] + 1):, :, :, :] = rows_negative_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapedata(df):\n",
    "    df=df.drop(['sample','Diagnosis'],axis=1)\n",
    "    for i in range(df.shape[0]):\n",
    "        x1=np.asarray(df.iloc[i:i+1, : 12500])\n",
    "        x1=x1.reshape(1,5,50,50,)\n",
    "        if i==0:\n",
    "            x_full=x1\n",
    "        else:\n",
    "            x_full=np.vstack((x_full,x1))\n",
    "    return x_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdpb_rs_pd=reshapedata(mod_dataPDBP[(mod_dataPDBP['Diagnosis']==\"PD\")])\n",
    "pdpb_rs_hc=reshapedata(mod_dataPDBP[(mod_dataPDBP['Diagnosis']==\"HC\")])\n",
    "#repeat the negatives to fill batch size\n",
    "pdpb_rs_pd=np.vstack((pdpb_rs_pd,pdpb_rs_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdpb_rs_hc[1:(config['positives_per_batch_count'] + 1),:,:,:].shape\n",
    "#pdpb_rs_pd[1:config['batch_size']-config['positives_per_batch_count'],:,:,:].shape\n",
    "#pdpb_rs_pd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_channels=5\n",
    "height=50\n",
    "width-50\n",
    "\n",
    "positives per batch count?\n",
    "How to define anchor, positive and negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator():\n",
    "    # each batch contains an anchor x at index 0 followed by\n",
    "    # config[\"positives_per_batch_count\"] positives followed by\n",
    "    # negatives.\n",
    "    \n",
    "     ### ==> TODO: You need to write this function for your case to use this code\n",
    "    train_ratio = .7\n",
    "    iters = 1e5\n",
    "\n",
    "    for i in range(iters):\n",
    "        batch_data = np.zeros([config['batch_size'],\n",
    "                               config['num_channels'],\n",
    "                               config['x_height'],\n",
    "                               config['x_width']], dtype = np.float32)\n",
    "\n",
    "        batch_data[0, :, :, :] = pdpb_rs_hc[0,:,:,:]#anchor_data        \n",
    "        batch_data[1:(config['positives_per_batch_count'] + 1), :, :, :] = pdpb_rs_hc[1:(config['positives_per_batch_count'] + 1),:,:,:]#positive_data\n",
    "        batch_data[(config['positives_per_batch_count'] + 1):, :, :, :] = pdpb_rs_pd[1:config['batch_size']-config['positives_per_batch_count'],:,:,:]#negative_data\n",
    "        \n",
    "        if i <= np.int(train_ratio * iters):\n",
    "            batch_type = 'train'\n",
    "        else:\n",
    "            batch_type = 'cv'\n",
    "        yield np.array(batch_data).tobytes(), batch_type\n",
    "        \n",
    "def batch_generator2():\n",
    "    # each batch contains an anchor x at index 0 followed by\n",
    "    # config[\"positives_per_batch_count\"] positives followed by\n",
    "    # negatives.\n",
    "    \n",
    "     ### ==> TODO: You need to write this function for your case to use this code\n",
    "    train_ratio = .7\n",
    "    iters = 10000\n",
    "\n",
    "    for _iter in range(iters):\n",
    "        batch_data = np.zeros([config['batch_size'],\n",
    "                               config['num_channels'],\n",
    "                               config['x_height'],\n",
    "                               config['x_width']], dtype = np.float32)\n",
    "\n",
    "        batch_data[0, :, :, :] = pdpb_rs_hc[0,:,:,:]#anchor_data        \n",
    "        batch_data[1:(config['positives_per_batch_count'] + 1), :, :, :] = pdpb_rs_hc[1:(config['positives_per_batch_count'] + 1),:,:,:]#positive_data\n",
    "        batch_data[(config['positives_per_batch_count'] + 1):, :, :, :] = pdpb_rs_pd[1:config['batch_size']-config['positives_per_batch_count'],:,:,:]#negative_data\n",
    "        \n",
    "        if _iter <= np.int(train_ratio * iters):\n",
    "            batch_type = 'train'\n",
    "        else:\n",
    "            batch_type = 'cv'\n",
    "        return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[16.283676 ,  5.85807  ,  9.80593  , ...,  6.620599 ,\n",
       "           6.9490438,  6.452058 ],\n",
       "         [10.1461735,  6.197455 ,  7.712767 , ...,  6.6899767,\n",
       "          13.227364 ,  7.157033 ],\n",
       "         [ 7.1282735,  7.722627 ,  6.88701  , ...,  7.31626  ,\n",
       "           7.864062 ,  7.6961412],\n",
       "         ...,\n",
       "         [ 6.6554956,  9.789783 ,  7.929084 , ...,  6.5559764,\n",
       "           8.219839 ,  6.079641 ],\n",
       "         [ 7.5781274,  7.658472 , 13.148856 , ...,  7.31626  ,\n",
       "          12.918608 ,  6.862342 ],\n",
       "         [ 6.696346 ,  6.977474 ,  8.873656 , ...,  6.3586397,\n",
       "           8.794668 ,  6.8299117]],\n",
       "\n",
       "        [[ 9.023597 ,  6.3764424,  6.8442   , ...,  9.336162 ,\n",
       "           6.3171825,  7.765627 ],\n",
       "         [ 6.620599 ,  5.828103 ,  6.3336554, ...,  6.6899767,\n",
       "           9.01916  ,  8.273027 ],\n",
       "         [ 7.781165 ,  7.7695465,  7.6727858, ...,  7.5076723,\n",
       "          10.684354 ,  6.1703415],\n",
       "         ...,\n",
       "         [ 6.1515822,  6.6554956,  6.3171825, ...,  8.20265  ,\n",
       "           8.00439  ,  5.9134755],\n",
       "         [ 6.452058 ,  9.550536 ,  7.4246426, ...,  9.067878 ,\n",
       "           6.5757103,  7.0901933],\n",
       "         [ 8.965374 ,  8.993303 ,  6.2818046, ...,  6.2077703,\n",
       "           7.106739 , 10.199513 ]],\n",
       "\n",
       "        [[ 6.6818004, 10.671103 ,  6.9227824, ...,  8.038465 ,\n",
       "           6.9820046,  8.62927  ],\n",
       "         [ 9.40044  ,  6.6554956,  9.716119 , ...,  7.919244 ,\n",
       "           8.539001 ,  7.9811234],\n",
       "         [ 6.9669857,  8.486415 ,  6.810489 , ...,  7.929084 ,\n",
       "           6.850668 ,  6.2077703],\n",
       "         ...,\n",
       "         [ 6.8681917,  8.846687 ,  5.315014 , ...,  6.3171825,\n",
       "           6.2818046, 10.076324 ],\n",
       "         [ 7.0526395, 11.344274 ,  9.071043 , ...,  6.9490438,\n",
       "           7.9431324,  6.1781793],\n",
       "         [ 8.309781 ,  6.7043824,  6.897383 , ...,  7.0901933,\n",
       "           8.205335 ,  9.10611  ]],\n",
       "\n",
       "        [[ 8.479029 ,  7.765627 ,  7.7695465, ...,  6.06259  ,\n",
       "           9.731114 ,  7.320611 ],\n",
       "         [ 6.485128 ,  8.030231 ,  8.790204 , ...,  7.1934958,\n",
       "           6.5757103,  6.1618433],\n",
       "         [ 6.197455 ,  7.757282 ,  7.490084 , ...,  9.631797 ,\n",
       "           6.111716 ,  7.87216  ],\n",
       "         ...,\n",
       "         [ 8.405043 ,  6.1515822, 10.741028 , ...,  6.8243957,\n",
       "          16.186823 ,  6.696346 ],\n",
       "         [ 9.927427 ,  8.607875 ,  6.3764424, ...,  6.4615436,\n",
       "          13.536833 ,  8.212688 ],\n",
       "         [ 7.5706735,  6.053708 ,  7.232643 , ...,  6.5685554,\n",
       "           6.273828 ,  5.7351875]],\n",
       "\n",
       "        [[ 7.921554 ,  7.757282 ,  6.64836  , ...,  8.984308 ,\n",
       "           6.3670354,  6.7838407],\n",
       "         [ 6.1703415,  6.021015 ,  7.1152673, ...,  8.184782 ,\n",
       "           6.053708 ,  8.379471 ],\n",
       "         [ 6.5559764,  6.7344546,  7.804985 , ...,  8.687114 ,\n",
       "           7.268661 ,  7.211723 ],\n",
       "         ...,\n",
       "         [ 7.341782 ,  7.138179 , 15.578037 , ..., 10.070063 ,\n",
       "          11.535404 ,  7.5706735],\n",
       "         [ 6.909874 ,  7.5189886,  9.352855 , ...,  6.6403904,\n",
       "           6.5559764,  6.88701  ],\n",
       "         [ 8.058017 ,  6.1515822,  7.1839614, ...,  8.242478 ,\n",
       "           7.5751643,  7.0682473]]],\n",
       "\n",
       "\n",
       "       [[[16.29392  ,  5.8094416,  9.954631 , ...,  6.7546325,\n",
       "           7.0353265,  6.5783653],\n",
       "         [10.396341 ,  6.3347197,  7.8377714, ...,  6.5457892,\n",
       "          12.036898 ,  7.243167 ],\n",
       "         [ 7.0985785,  7.76791  ,  7.3692617, ...,  7.4557295,\n",
       "           7.8781295,  7.599254 ],\n",
       "         ...,\n",
       "         [ 6.7053685,  9.824888 ,  8.095316 , ...,  6.6211567,\n",
       "           8.046886 ,  6.053638 ],\n",
       "         [ 7.364426 ,  7.72862  , 12.508488 , ...,  7.4057903,\n",
       "          12.377617 ,  6.731545 ],\n",
       "         [ 6.6155443,  6.9599466,  8.950091 , ...,  6.4067955,\n",
       "           8.049617 ,  6.5313025]],\n",
       "\n",
       "        [[ 9.347668 ,  6.5185714,  6.99156  , ...,  9.432127 ,\n",
       "           6.6782894,  7.7106595],\n",
       "         [ 6.5457892,  5.7844157,  6.222888 , ...,  6.5901375,\n",
       "           8.952089 ,  8.326037 ],\n",
       "         [ 7.6414323,  8.029178 ,  7.9418087, ...,  7.5473886,\n",
       "          10.514165 ,  6.2706747],\n",
       "         ...,\n",
       "         [ 5.983704 ,  6.88229  ,  6.5839663, ...,  8.395448 ,\n",
       "           8.024772 ,  5.983704 ],\n",
       "         [ 6.359659 ,  9.763654 ,  7.5154715, ...,  9.052584 ,\n",
       "           6.7775803,  7.043434 ],\n",
       "         [ 9.33134  ,  9.105324 ,  6.443515 , ...,  6.3993745,\n",
       "           7.235092 , 10.333481 ]],\n",
       "\n",
       "        [[ 6.9000397, 10.848658 ,  7.020759 , ...,  7.7847805,\n",
       "           7.0803404,  8.898208 ],\n",
       "         [ 9.625369 ,  6.7490053,  9.564601 , ...,  7.9254823,\n",
       "           8.467389 ,  7.840915 ],\n",
       "         [ 7.047453 ,  8.695745 ,  7.323831 , ...,  7.6214027,\n",
       "           6.8772   ,  6.2950177],\n",
       "         ...,\n",
       "         [ 6.9489427,  8.761172 ,  5.7470746, ...,  6.246468 ,\n",
       "           6.5724354, 10.180374 ],\n",
       "         [ 7.128806 , 11.522819 ,  9.143752 , ...,  6.8892975,\n",
       "           7.8826976,  6.1574287],\n",
       "         [ 8.364119 ,  6.5839663,  6.915329 , ...,  7.1030283,\n",
       "           7.91313  ,  9.224835 ]],\n",
       "\n",
       "        [[ 8.21658  ,  7.808469 ,  7.7362447, ...,  5.8308125,\n",
       "           9.587557 ,  7.4192085],\n",
       "         [ 6.4620237,  7.972514 ,  8.449151 , ...,  7.11109  ,\n",
       "           6.9275966,  6.2379384],\n",
       "         [ 5.86396  ,  7.811022 ,  7.754931 , ...,  9.633778 ,\n",
       "           6.053638 ,  8.064044 ],\n",
       "         ...,\n",
       "         [ 8.751946 ,  6.116577 , 10.510339 , ...,  6.88229  ,\n",
       "          16.186823 ,  6.5839663],\n",
       "         [10.017222 ,  8.50568  ,  6.2950177, ...,  6.4893284,\n",
       "          13.457422 ,  7.5293617],\n",
       "         [ 7.536365 ,  5.798709 ,  7.1645885, ...,  6.6635985,\n",
       "           6.2379384,  5.7470746]],\n",
       "\n",
       "        [[ 8.078678 ,  7.8826976,  6.5537724, ...,  9.008364 ,\n",
       "           6.2851686,  6.374553 ],\n",
       "         [ 6.141653 ,  6.026206 ,  7.1030283, ...,  8.055297 ,\n",
       "           5.99281  ,  8.443257 ],\n",
       "         [ 5.694031 ,  6.7490053,  7.3399944, ...,  8.545214 ,\n",
       "           7.4557295,  7.5441236],\n",
       "         ...,\n",
       "         [ 7.4291167,  7.2557273, 14.866337 , ...,  9.503405 ,\n",
       "          11.149206 ,  7.6150665],\n",
       "         [ 6.7884893,  7.1896524,  9.30327  , ...,  6.3906693,\n",
       "           6.4067955,  7.029791 ],\n",
       "         [ 7.9296527,  6.561414 ,  7.307905 , ...,  8.106408 ,\n",
       "           7.504088 ,  6.7113376]]],\n",
       "\n",
       "\n",
       "       [[[16.380302 ,  5.7776217,  9.909529 , ...,  6.8937936,\n",
       "           6.8777285,  6.379813 ],\n",
       "         [10.188095 ,  6.1442513,  7.7874875, ...,  7.1314774,\n",
       "          12.904624 ,  7.3341413],\n",
       "         [ 7.20274  ,  8.009988 ,  6.9986825, ...,  7.070756 ,\n",
       "           7.777438 ,  7.674426 ],\n",
       "         ...,\n",
       "         [ 6.788611 ,  9.906839 ,  7.266102 , ...,  6.320676 ,\n",
       "           8.166538 ,  5.8568344],\n",
       "         [ 7.4293613,  7.7533484, 12.859978 , ...,  7.736484 ,\n",
       "          12.571752 ,  6.9061546],\n",
       "         [ 6.6281924,  6.9986825,  8.785198 , ...,  6.266784 ,\n",
       "           8.8620205,  6.81945  ]],\n",
       "\n",
       "        [[ 8.689242 ,  6.294633 ,  6.744901 , ...,  9.672064 ,\n",
       "           5.9910455,  7.817565 ],\n",
       "         [ 6.870941 ,  5.9019413,  6.320676 , ...,  6.369765 ,\n",
       "           9.106626 ,  8.45824  ],\n",
       "         [ 7.4980845,  7.5343657,  7.662072 , ...,  7.674426 ,\n",
       "          10.310971 ,  6.1014876],\n",
       "         ...,\n",
       "         [ 6.294633 ,  6.599655 ,  6.257778 , ...,  8.296054 ,\n",
       "           8.078548 ,  5.978303 ],\n",
       "         [ 6.5531864,  9.465106 ,  7.726771 , ...,  8.740509 ,\n",
       "           6.8336945,  6.9853435],\n",
       "         [ 9.030893 ,  8.907699 ,  6.615309 , ...,  6.121109 ,\n",
       "           7.3310027, 10.133249 ]],\n",
       "\n",
       "        [[ 6.788611 , 10.111355 ,  6.788611 , ...,  7.9089537,\n",
       "           6.841153 ,  8.430496 ],\n",
       "         [ 9.72536  ,  6.7948327,  9.586467 , ...,  7.9660892,\n",
       "           8.178115 ,  7.853179 ],\n",
       "         [ 6.9263854,  8.463463 ,  6.8653116, ...,  7.918913 ,\n",
       "           6.6852336,  6.43995  ],\n",
       "         ...,\n",
       "         [ 6.788611 ,  8.988557 ,  5.7776217, ...,  6.246867 ,\n",
       "           6.4705515,  9.9432335],\n",
       "         [ 6.979807 , 11.404992 ,  9.158954 , ...,  6.979807 ,\n",
       "           8.063916 ,  6.276469 ],\n",
       "         [ 8.407243 ,  6.744901 ,  7.1150804, ...,  7.088883 ,\n",
       "           8.160787 ,  9.26895  ]],\n",
       "\n",
       "        [[ 8.353179 ,  8.0383835,  7.944976 , ...,  6.0116343,\n",
       "           9.845894 ,  7.213928 ],\n",
       "         [ 6.2259645,  8.218751 ,  8.802996 , ...,  6.9506044,\n",
       "           6.7948327,  6.166432 ],\n",
       "         [ 6.0331516,  7.674426 ,  7.6704826, ...,  9.552906 ,\n",
       "           6.0672407,  7.9963746],\n",
       "         ...,\n",
       "         [ 8.854539 ,  6.0116343, 10.319831 , ...,  6.8018003,\n",
       "          16.163433 ,  6.607381 ],\n",
       "         [ 9.855393 ,  8.313308 ,  6.2849474, ...,  6.479255 ,\n",
       "          13.377274 ,  8.214766 ],\n",
       "         [ 7.4343386,  5.966818 ,  7.2611456, ...,  6.6625175,\n",
       "           6.1869316,  5.7627263]],\n",
       "\n",
       "        [[ 7.806532 ,  7.918913 ,  6.5627966, ...,  9.16247  ,\n",
       "           6.3951874,  6.870941 ],\n",
       "         [ 6.0899067,  6.121109 ,  7.0512357, ...,  8.224015 ,\n",
       "           5.9527273,  8.448339 ],\n",
       "         [ 6.1869316,  6.6482573,  7.3117943, ...,  8.4661665,\n",
       "           7.4343386,  7.5373464],\n",
       "         ...,\n",
       "         [ 7.208178 ,  7.105264 , 15.17943  , ...,  9.689467 ,\n",
       "          11.256682 ,  7.7439556],\n",
       "         [ 6.8937936,  7.6704826,  9.4938755, ...,  6.722894 ,\n",
       "           6.5627966,  6.788611 ],\n",
       "         [ 7.8344965,  6.246867 ,  7.326574 , ...,  8.363269 ,\n",
       "           7.481068 ,  6.788611 ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[16.360777 ,  5.9229093, 10.29021  , ...,  6.4071956,\n",
       "           7.5527115,  6.7025437],\n",
       "         [ 9.329861 ,  6.1855907,  7.1823907, ...,  6.569803 ,\n",
       "          10.62859  ,  6.9849877],\n",
       "         [ 7.2811003,  7.8559422,  7.5499864, ...,  7.85982  ,\n",
       "           8.123094 ,  7.352997 ],\n",
       "         ...,\n",
       "         [ 7.2547894,  9.198899 ,  8.046228 , ...,  6.737276 ,\n",
       "           8.133554 ,  6.134486 ],\n",
       "         [ 7.441538 ,  7.8470874, 11.090521 , ...,  7.0174274,\n",
       "          11.413607 ,  6.940393 ],\n",
       "         [ 6.614473 ,  6.9849877,  9.151514 , ...,  6.421948 ,\n",
       "           7.542024 ,  6.3923583]],\n",
       "\n",
       "        [[ 9.183594 ,  6.5752745,  6.818619 , ..., 10.054511 ,\n",
       "           6.226436 ,  7.789786 ],\n",
       "         [ 6.5597277,  5.800464 ,  6.548616 , ...,  7.2269573,\n",
       "           9.310169 ,  8.153801 ],\n",
       "         [ 7.8427567,  8.22727  ,  7.9284577, ...,  7.542024 ,\n",
       "          10.889627 ,  6.2531476],\n",
       "         ...,\n",
       "         [ 6.265474 ,  6.729635 ,  6.3413286, ...,  7.826845 ,\n",
       "           7.9351487,  5.8799977],\n",
       "         [ 6.515542 ,  9.539472 ,  7.099673 , ...,  8.4562235,\n",
       "           6.589002 ,  7.343274 ],\n",
       "         [ 9.20575  ,  8.037511 ,  6.6353254, ...,  6.46899  ,\n",
       "           7.2086053, 10.753616 ]],\n",
       "\n",
       "        [[ 6.798762 ,  9.586826 ,  6.7025437, ...,  8.053783 ,\n",
       "           7.3459773,  9.940573 ],\n",
       "         [10.085269 ,  6.9750395, 10.2152   , ...,  7.60299  ,\n",
       "           7.785275 ,  7.557827 ],\n",
       "         [ 7.4305077,  8.674287 ,  7.2351794, ...,  7.596407 ,\n",
       "           7.3615146,  6.7074065],\n",
       "         ...,\n",
       "         [ 6.6246057,  7.7664614,  5.86468  , ...,  6.421948 ,\n",
       "           6.713395 , 10.190327 ],\n",
       "         [ 7.1998763, 11.556889 ,  9.762427 , ...,  6.6524177,\n",
       "           7.4758735,  6.3923583],\n",
       "         [ 8.241521 ,  6.7722025,  7.1594057, ...,  7.1998763,\n",
       "           8.883281 ,  9.24997  ]],\n",
       "\n",
       "        [[ 8.083619 ,  8.051519 ,  7.396457 , ...,  6.1160827,\n",
       "           9.394962 ,  7.1906357],\n",
       "         [ 7.4305077,  7.831282 ,  9.498432 , ...,  6.921186 ,\n",
       "           6.733652 ,  6.1855907],\n",
       "         [ 6.1855907,  7.988473 ,  7.175596 , ...,  9.419169 ,\n",
       "           5.995496 ,  8.103484 ],\n",
       "         ...,\n",
       "         [ 9.659658 ,  6.1434994, 10.436044 , ...,  6.5319057,\n",
       "          16.070156 ,  6.823657 ],\n",
       "         [10.225142 ,  8.086361 ,  6.2388363, ...,  6.537703 ,\n",
       "          13.527672 ,  7.888942 ],\n",
       "         [ 7.589566 ,  6.134486 ,  7.249161 , ...,  7.0624223,\n",
       "           6.3535657,  5.7066507]],\n",
       "\n",
       "        [[ 7.85982  ,  8.290233 ,  7.106496 , ...,  8.646994 ,\n",
       "           6.8678827,  6.46899  ],\n",
       "         [ 5.995496 ,  6.004263 ,  7.350879 , ...,  8.112487 ,\n",
       "           5.9451823,  7.849237 ],\n",
       "         [ 5.7354717,  6.6577682,  6.921186 , ...,  8.536204 ,\n",
       "           7.276599 ,  7.441538 ],\n",
       "         ...,\n",
       "         [ 7.5149803,  7.3271813, 13.17084  , ..., 10.029718 ,\n",
       "          10.428208 ,  7.355494 ],\n",
       "         [ 6.8786764,  6.884141 ,  9.529392 , ...,  6.4751635,\n",
       "           6.4933186,  7.268983 ],\n",
       "         [ 8.29973  ,  6.0699253,  7.7383904, ...,  7.5264907,\n",
       "           7.251958 ,  6.614473 ]]],\n",
       "\n",
       "\n",
       "       [[[16.385271 ,  6.0049934,  9.750757 , ...,  6.761004 ,\n",
       "           6.7811985,  6.5391564],\n",
       "         [ 9.891479 ,  6.240748 ,  7.7146716, ...,  6.6908007,\n",
       "          12.303425 ,  7.0236073],\n",
       "         [ 7.099891 ,  7.7871327,  7.4775395, ...,  7.6532564,\n",
       "           8.173235 ,  7.695163 ],\n",
       "         ...,\n",
       "         [ 6.7223015,  9.4574375,  7.707796 , ...,  6.4424224,\n",
       "           8.312853 ,  6.026206 ],\n",
       "         [ 7.695163 ,  8.02605  , 12.12668  , ...,  7.343752 ,\n",
       "          12.377617 ,  7.10562  ],\n",
       "         [ 6.627209 ,  7.1649036,  8.960746 , ...,  6.34537  ,\n",
       "           8.497155 ,  6.5391564]],\n",
       "\n",
       "        [[ 8.32649  ,  6.378499 ,  6.673118 , ...,  9.664168 ,\n",
       "           6.05131  ,  7.49161  ],\n",
       "         [ 6.5842214,  5.727915 ,  6.2947774, ...,  6.9709787,\n",
       "           8.982063 ,  8.039925 ],\n",
       "         [ 7.9831505,  7.452503 ,  7.6696286, ...,  7.5973268,\n",
       "          11.002335 ,  6.1711817],\n",
       "         ...,\n",
       "         [ 6.283132 ,  6.809776 ,  6.913246 , ...,  7.7833576,\n",
       "           8.259449 ,  6.07736  ],\n",
       "         [ 6.7530894,  9.053813 ,  7.325768 , ...,  8.713972 ,\n",
       "           6.8390894,  7.2025447],\n",
       "         [ 8.762884 ,  8.727174 ,  6.7968407, ...,  6.6422234,\n",
       "           7.2550716, 10.308153 ]],\n",
       "\n",
       "        [[ 6.892703 ,  9.967075 ,  7.389271 , ...,  8.184782 ,\n",
       "           7.191288 ,  8.493962 ],\n",
       "         [ 9.508735 ,  6.7530894,  9.947509 , ...,  7.920348 ,\n",
       "           8.33306  ,  7.7871327],\n",
       "         [ 6.7968407,  8.0994215,  6.8176813, ...,  8.115306 ,\n",
       "           6.657877 ,  6.3673506],\n",
       "         ...,\n",
       "         [ 6.627209 ,  8.276247 ,  5.7603517, ...,  6.3062525,\n",
       "           6.3183026, 10.498479 ],\n",
       "         [ 6.7811985, 11.548336 ,  8.919186 , ...,  6.883852 ,\n",
       "           7.7558484,  6.34537  ],\n",
       "         [ 8.500706 ,  6.523049 ,  7.030075 , ...,  7.1479206,\n",
       "           8.191816 ,  8.929482 ]],\n",
       "\n",
       "        [[ 8.354706 ,  8.002423 ,  7.6123805, ...,  6.1025186,\n",
       "           9.499476 ,  7.3024716],\n",
       "         [ 6.657877 ,  7.9965286,  9.153621 , ...,  6.9644017,\n",
       "           6.8390894,  6.34537  ],\n",
       "         [ 6.1711817,  8.166234 ,  7.10562  , ...,  9.600173 ,\n",
       "           5.981823 ,  7.8622246],\n",
       "         ...,\n",
       "         [ 8.885902 ,  6.124914 , 10.376843 , ...,  6.824539 ,\n",
       "          16.240513 ,  6.9270735],\n",
       "         [10.29208  ,  8.265517 ,  6.240748 , ...,  6.715668 ,\n",
       "          13.670143 ,  7.389271 ],\n",
       "         [ 7.9306593,  5.904855 ,  6.9400773, ...,  6.736461 ,\n",
       "           6.0049934,  5.727915 ]],\n",
       "\n",
       "        [[ 7.5973268,  8.318102 ,  6.408035 , ...,  8.943866 ,\n",
       "           6.4915996,  6.809776 ],\n",
       "         [ 6.1970143,  5.981823 ,  7.042548 , ...,  8.411709 ,\n",
       "           5.775413 ,  8.330018 ],\n",
       "         [ 6.452447 ,  6.736461 ,  8.093049 , ...,  8.588948 ,\n",
       "           7.0814543,  7.380628 ],\n",
       "         ...,\n",
       "         [ 6.991295 ,  6.673118 , 14.418885 , ..., 10.217318 ,\n",
       "          11.260901 ,  7.5973268],\n",
       "         [ 6.933436 ,  7.191288 ,  9.731556 , ...,  6.613931 ,\n",
       "           6.591004 ,  6.657877 ],\n",
       "         [ 8.336828 ,  6.262197 ,  7.0056686, ...,  7.920348 ,\n",
       "           7.847763 ,  6.7968407]]],\n",
       "\n",
       "\n",
       "       [[[16.320719 ,  6.1682086, 10.376843 , ...,  7.0350657,\n",
       "           7.2249045,  7.4618583],\n",
       "         [10.150429 ,  6.603664 ,  8.074816 , ...,  6.7382617,\n",
       "          11.660365 ,  7.2939625],\n",
       "         [ 7.4618583,  7.7738557,  7.9722514, ...,  7.586571 ,\n",
       "           9.02165  ,  7.8706803],\n",
       "         ...,\n",
       "         [ 6.877348 ,  8.921749 ,  8.096558 , ...,  6.832215 ,\n",
       "           9.036527 ,  6.0836964],\n",
       "         [ 7.8047857,  7.643402 , 12.066852 , ...,  7.2249045,\n",
       "          12.1221   ,  7.199645 ],\n",
       "         [ 7.140539 ,  6.9242897,  9.448276 , ...,  6.4797153,\n",
       "           7.3448687,  6.5435896]],\n",
       "\n",
       "        [[ 9.317273 ,  6.3925858,  6.7020807, ...,  9.036527 ,\n",
       "           6.7020807,  8.697436 ],\n",
       "         [ 6.5435896,  6.0255814,  6.321708 , ...,  6.756884 ,\n",
       "           9.16466  ,  9.016966 ],\n",
       "         [ 8.333928 ,  8.096558 ,  8.829555 , ...,  7.0927553,\n",
       "          10.2053   ,  6.1965756],\n",
       "         ...,\n",
       "         [ 6.223942 ,  6.8612223,  6.2752523, ...,  8.001842 ,\n",
       "           8.401035 ,  5.941755 ],\n",
       "         [ 6.321708 ,  9.233087 ,  7.0821056, ...,  9.20128  ,\n",
       "           7.007754 ,  6.8925357],\n",
       "         [ 9.287503 ,  8.897139 ,  6.3457384, ...,  6.223942 ,\n",
       "           7.271756 , 10.556822 ]],\n",
       "\n",
       "        [[ 7.007754 , 10.595898 ,  6.4996943, ...,  7.586571 ,\n",
       "           6.7382617,  8.061552 ],\n",
       "         [ 9.140602 ,  6.7020807,  9.279061 , ...,  7.54853  ,\n",
       "           8.35757  ,  7.586571 ],\n",
       "         [ 6.7020807,  8.026598 ,  6.9242897, ...,  6.981184 ,\n",
       "           7.417008 ,  6.5435896],\n",
       "         ...,\n",
       "         [ 6.5435896,  7.7645783,  5.747999 , ...,  6.5210967,\n",
       "           6.321708 ,  9.714504 ],\n",
       "         [ 6.7746406, 10.694368 ,  8.829555 , ...,  6.5435896,\n",
       "           7.529899 ,  6.223942 ],\n",
       "         [ 7.9012456,  7.7738557,  7.0350657, ...,  7.164015 ,\n",
       "           9.219204 ,  9.206867 ]],\n",
       "\n",
       "        [[ 8.697436 ,  7.96089  ,  7.6846433, ...,  5.653607 ,\n",
       "           9.11407  ,  6.7382617],\n",
       "         [ 6.5842214,  8.541505 ,  9.835948 , ...,  6.7020807,\n",
       "           6.321708 ,  6.1682086],\n",
       "         [ 5.99849  ,  7.643402 ,  7.643402 , ...,  8.683671 ,\n",
       "           5.882567 ,  8.333928 ],\n",
       "         ...,\n",
       "         [ 8.497155 ,  6.223942 , 11.407265 , ...,  6.4148827,\n",
       "          16.3026   ,  6.663024 ],\n",
       "         [ 9.722645 ,  8.010132 ,  6.1124105, ...,  6.8447056,\n",
       "          12.62054  ,  8.1617565],\n",
       "         [ 7.5182195,  5.6932974,  6.8612223, ...,  6.5435896,\n",
       "           6.3686285,  5.7776217]],\n",
       "\n",
       "        [[ 8.068244 ,  8.35757  ,  6.623714 , ...,  8.409134 ,\n",
       "           6.251427 ,  6.4571476],\n",
       "         [ 5.882567 ,  6.0543613,  7.2249045, ...,  8.096558 ,\n",
       "           5.7776217,  7.7832313],\n",
       "         [ 2.8258922,  5.941755 ,  6.7382617, ...,  8.12703  ,\n",
       "           7.8706803,  7.8850245],\n",
       "         ...,\n",
       "         [ 7.3948483,  7.199645 , 14.362722 , ..., 10.029718 ,\n",
       "          11.0654335,  7.211921 ],\n",
       "         [ 7.2939625,  7.7217927, 10.403183 , ...,  6.2752523,\n",
       "           6.3686285,  6.981184 ],\n",
       "         [ 8.697436 ,  5.8281827,  7.0927553, ...,  7.271756 ,\n",
       "           7.643402 ,  7.140539 ]]]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_generator2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ratio = .7\n",
    "iters = 1e5\n",
    "np.int(train_ratio * iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " def write_to_tfrecord():\n",
    "    # write batches to tfrecords file\n",
    "    data_generator = batch_generator()\n",
    "    writer_train = tf.python_io.TFRecordWriter(config['tfrecords_train'])\n",
    "    writer_cv = tf.python_io.TFRecordWriter(config['tfrecords_cv'])\n",
    "    train_counter = 0\n",
    "    cv_counter = 0\n",
    "    try:\n",
    "        while(1):\n",
    "            batch, batch_type = next(data_generator)\n",
    "            feature = tf.train.Feature(bytes_list = tf.train.BytesList(value = [batch]))\n",
    "            feature_dict = {'batch':\n",
    "                tf.train.Feature(bytes_list = tf.train.BytesList(value = [batch]))}\n",
    "            example = tf.train.Example(features = tf.train.Features(feature = feature_dict))\n",
    "            if batch_type == 'train':\n",
    "                train_counter += 1\n",
    "                writer_train.write(example.SerializeToString())\n",
    "                print('>>>>>> train: ', train_counter, end = '\\r')\n",
    "            else:\n",
    "                cv_counter += 1\n",
    "                writer_cv.write(example.SerializeToString())\n",
    "                print('>>>>>> cv: ', cv_counter, end = '\\r')\n",
    "    except:\n",
    "        print('')\n",
    "    finally:\n",
    "        print('total_number of trained records:', train_counter)\n",
    "        print('total_number of cv records:', cv_counter)\n",
    "        writer_train.close()\n",
    "        writer_cv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_tfrecord(tfrecord_file, queue_size, num_threads, min_capacity):\n",
    "    # read batch from tfrecords file\n",
    "    with tf.variable_scope('Queue_Batch_Shuffle', reuse = False) as scope:\n",
    "        tfrecord_file_queue = tf.train.string_input_producer(tfrecord_file, name = 'queue')\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, tfrecord_serialized = reader.read(tfrecord_file_queue)\n",
    "        tfrecord_features = tf.parse_single_example(tfrecord_serialized,\n",
    "                    features = {'batch': tf.FixedLenFeature([], tf.string)},\n",
    "                    name = 'features')\n",
    "\n",
    "        batch_data = tf.decode_raw(tfrecord_features['batch'], tf.float32)\n",
    "        batch_data = tf.reshape(batch_data, [config['batch_size'],\n",
    "                                             config['num_channels'],\n",
    "                                             config['x_height'],\n",
    "                                             config['x_width']])\n",
    "\n",
    "        batch_data_shuffled = tf.train.shuffle_batch([batch_data],\n",
    "                                                      batch_size = 1,\n",
    "                                                      capacity = queue_size,\n",
    "                                                      num_threads = num_threads,\n",
    "                                                      min_after_dequeue = min_capacity)\n",
    "        return batch_data_shuffled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_list_maker():\n",
    "    # make a list of <anchor_index, positive_index, negativeIndex> triplets\n",
    "    # of size config['triplets_count']\n",
    "    triplets = []\n",
    "    negatives_per_batch_count = config['batch_size'] - config['positives_per_batch_count']\n",
    "    positives_index = list(range(config['positives_per_batch_count']))\n",
    "    np.random.shuffle(positives_index)\n",
    "    for positive_idx in positives_index:\n",
    "        pos_idx = positive_idx + 1\n",
    "        negatives_index = list(range(negatives_per_batch_count))\n",
    "        np.random.shuffle(negatives_index)\n",
    "        for negative_idx in negatives_index:\n",
    "            neg_idx = negative_idx + 1 + config['positives_per_batch_count']\n",
    "            triplets.append([0, pos_idx, neg_idx])\n",
    "            if len(triplets) == config['triplets_count']:\n",
    "                result = np.array(triplets)\n",
    "                np.random.shuffle(result)\n",
    "                return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 26, 26, 2)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "input_shape = (4, 28, 28, 3)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.Conv2D(\n",
    "2, 3, activation='relu', input_shape=input_shape[1:])(x)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x, dropout_rate = [0.1, 0.05], is_training = True, print_summary = False):\n",
    "    # build the CNN model\n",
    "    print('Build model...')\n",
    "    with tf.variable_scope('Model', reuse = False) as scope:\n",
    "        # block 1\n",
    "        conv1 = tf.layers.conv2d(x, 25, [3, 3],\n",
    "                                 strides = [1, 1],\n",
    "                                 data_format = 'channels_first',\n",
    "                                 padding = 'same',\n",
    "                                 name = 'conv1')\n",
    "        batch_norm1 = tf.layers.batch_normalization(conv1, training = is_training,\n",
    "                                                    axis = 1, name = 'batch_norm1')\n",
    "        relu1 = tf.nn.relu(batch_norm1, name = 'relu1')\n",
    "        max_pool1 = tf.layers.max_pooling2d(relu1, [2, 2],\n",
    "                                            strides = [2, 2],\n",
    "                                            data_format = 'channels_first',\n",
    "                                            padding = 'valid',\n",
    "                                            name = 'max_pool1')\n",
    "\n",
    "        # block 2\n",
    "        conv2 = tf.layers.conv2d(max_pool1, 50, [2, 2],\n",
    "                                 strides = [1, 1],\n",
    "                                 data_format = 'channels_first',\n",
    "                                 padding = 'same',\n",
    "                                 name = 'conv2')\n",
    "        batch_norm2 = tf.layers.batch_normalization(conv2, training = is_training,\n",
    "                                                    axis = 1, name = 'batch_norm2')\n",
    "        relu2 = tf.nn.relu(batch_norm2, name = 'relu2')\n",
    "        max_pool2 = tf.layers.max_pooling2d(relu2, [2, 2],\n",
    "                                            strides = [2, 2],\n",
    "                                            data_format = 'channels_first',\n",
    "                                            padding = 'valid',\n",
    "                                            name = 'max_pool2')\n",
    "\n",
    "        # block 3\n",
    "        conv3 = tf.layers.conv2d(max_pool2, 100, [3, 3],\n",
    "                                 strides = [1, 1],\n",
    "                                 data_format = 'channels_first',\n",
    "                                 padding = 'same',\n",
    "                                 name = 'conv3')\n",
    "        batch_norm3 = tf.layers.batch_normalization(conv3, training = is_training,\n",
    "                                                    axis = 1, name = 'batch_norm3')\n",
    "        relu3 = tf.nn.relu(batch_norm3, name = 'relu3')\n",
    "        max_pool3 = tf.layers.max_pooling2d(relu3, [2, 2],\n",
    "                                            strides = [2, 2],\n",
    "                                            data_format = 'channels_first',\n",
    "                                            padding = 'valid',\n",
    "                                            name = 'max_pool3')\n",
    "\n",
    "        # block 4\n",
    "        conv4 = tf.layers.conv2d(max_pool3, 100, [2, 2],\n",
    "                                 strides = [1, 1],\n",
    "                                 data_format = 'channels_first',\n",
    "                                 padding = 'same',\n",
    "                                 name = 'conv4')\n",
    "        batch_norm4 = tf.layers.batch_normalization(conv4, training = is_training,\n",
    "                                                    axis = 1, name = 'batch_norm4')\n",
    "        relu4 = tf.nn.relu(batch_norm4, name = 'relu4')\n",
    "        max_pool4 = tf.layers.max_pooling2d(relu4, [2, 2],\n",
    "                                            strides = [2, 2],\n",
    "                                            data_format = 'channels_first',\n",
    "                                            padding = 'valid',\n",
    "                                            name = 'max_pool4')\n",
    "        dropout4 = tf.layers.dropout(max_pool4, dropout_rate[0], name = 'dropout4')\n",
    "\n",
    "        # block 5\n",
    "        flatten_length = dropout4.get_shape().as_list()[1] * \\\n",
    "                         dropout4.get_shape().as_list()[2] * \\\n",
    "                         dropout4.get_shape().as_list()[3]\n",
    "        flatten5 = tf.reshape(dropout4, (-1, flatten_length), name = 'flatten5')\n",
    "        fc5 = tf.layers.dense(flatten5, 1024, name = 'fc5')\n",
    "        batch_norm5 = tf.layers.batch_normalization(fc5, training = is_training,\n",
    "                                                    name = 'batch_norm5')\n",
    "        relu5 = tf.nn.relu(batch_norm5, name = 'relu5')\n",
    "\n",
    "        # block 6\n",
    "        fc6 = tf.layers.dense(relu5, 1024, name = 'fc6')\n",
    "        batch_norm6 = tf.layers.batch_normalization(fc6, training = is_training,\n",
    "                                                    name = 'batch_norm6')\n",
    "        relu6 = tf.nn.relu(batch_norm6, name = 'relu6')\n",
    "\n",
    "        # block 7\n",
    "        fc7 = tf.layers.dense(relu6, config['embedding_size'], name = 'fc7')\n",
    "        batch_norm7 = tf.layers.batch_normalization(fc7, training = is_training,\n",
    "                                                    name = 'batch_norm7')\n",
    "        relu7 = tf.nn.relu(batch_norm7, name = 'relu7')\n",
    "        dropout7 = tf.layers.dropout(relu7, dropout_rate[1], name = 'dropout7')\n",
    "        l27 = tf.nn.l2_normalize(fc7, 1, name ='l27')\n",
    "\n",
    "        # block 8\n",
    "        fc8 = tf.layers.dense(dropout7, config['embedding_size'], name = 'fc8')\n",
    "        l28 = tf.nn.l2_normalize(fc8, 1, name ='l28')\n",
    "\n",
    "        assert fc8.get_shape()[1] == config['embedding_size']\n",
    "        if print_summary:\n",
    "            print('Model summary:\\n x: %s\\n' \\\n",
    "                  ' conv1: %s\\n max_pool1: %s\\n' \\\n",
    "                  ' conv2: %s\\n max_pool2: %s\\n' \\\n",
    "                  ' conv3: %s\\n max_pool3: %s\\n' \\\n",
    "                  ' conv4: %s\\n max_pool4 %s\\n' \\\n",
    "                  ' flatten5 %s\\n fc5 %s\\n' \\\n",
    "                  ' fc6 %s\\n fc7 %s\\n' \\\n",
    "                  ' fc8 %s\\n' %(x.get_shape(),\n",
    "                                conv1.get_shape(), max_pool1.get_shape(),\n",
    "                                conv2.get_shape(), max_pool2.get_shape(),\n",
    "                                conv3.get_shape(), max_pool3.get_shape(),\n",
    "                                conv4.get_shape(), max_pool4.get_shape(),\n",
    "                                flatten5.get_shape(), fc5.get_shape(),\n",
    "                                fc6.get_shape(), fc7.get_shape(), fc8.get_shape()))\n",
    "        return l27, l28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(loss):\n",
    "    # model optimizer\n",
    "     with tf.variable_scope('Optimizer', reuse = False) as scope:\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            all_vars = tf.trainable_variables()\n",
    "            model_vars = [var for var in all_vars if var.name.startswith('Model')]\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate = config['learning_rate']).minimize(loss, var_list = model_vars)\n",
    "            return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_triplet_embeddings(triplets, embeddings):\n",
    "    # prepare triplet embeddings\n",
    "    # triplets: (anchor_index, positive_index, negative_index)\n",
    "\n",
    "    with tf.variable_scope('Embeddings', reuse = False) as scope:\n",
    "        anchor_embeddings = tf.gather(embeddings, triplets[:, 0])\n",
    "        positive_embeddings = tf.gather(embeddings, triplets[:, 1])        \n",
    "        negative_embeddings = tf.gather(embeddings, triplets[:, 2])\n",
    "\n",
    "        return anchor_embeddings, positive_embeddings, negative_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss_acc(triplet_embeddings1, triplet_embeddings2, mask, threshold):\n",
    "    # triplet loss that is being minimized:\n",
    "    # loss = reduce_mean(l2_norm_squared(f(x_a), f(x_p)) -\n",
    "    #                    l2_norm_squared(f(x_a), f(x_n)) +\n",
    "    #                    alpha)\n",
    "\n",
    "    anchor_embeddings1, positive_embeddings1, nagative_embeddings1 = triplet_embeddings1\n",
    "    anchor_embeddings2, positive_embeddings2, nagative_embeddings2 = triplet_embeddings2\n",
    "\n",
    "    with tf.variable_scope('Loss', reuse = False) as scope:\n",
    "        ap_dist2 = tf.reduce_sum(tf.square(anchor_embeddings2 - positive_embeddings2), axis = -1)\n",
    "        an_dist2 = tf.reduce_sum(tf.square(anchor_embeddings2 - nagative_embeddings2), axis = -1)\n",
    "\n",
    "        flags = tf.cast(tf.greater(an_dist2, ap_dist2), tf.float32)\n",
    "        flags = tf.maximum(flags, mask) # let a few ap > an cases skip and be used in training (for exploration)\n",
    "        base_loss2  = (ap_dist2 - an_dist2 + config['alpha_margin']) * flags\n",
    "        loss2 = tf.reduce_sum(tf.maximum(base_loss2, 0))\n",
    "\n",
    "        ap_dist1 = tf.reduce_sum(tf.square(anchor_embeddings1 - positive_embeddings1), axis = -1)\n",
    "        an_dist1 = tf.reduce_sum(tf.square(anchor_embeddings1 - nagative_embeddings1), axis = -1)\n",
    "        base_loss1  = (ap_dist1 - an_dist1 + config['alpha_margin']) * flags\n",
    "        loss1 = tf.reduce_sum(tf.maximum(base_loss1, 0))\n",
    "\n",
    "        loss = loss1 + loss2\n",
    "\n",
    "        ap_acc = tf.reduce_mean(tf.cast(tf.greater(threshold, tf.sqrt(ap_dist2)), tf.float32))\n",
    "        an_acc = tf.reduce_mean(tf.cast(tf.greater(tf.sqrt(an_dist2), threshold), tf.float32))\n",
    "        triplets_used = tf.reduce_sum(flags)\n",
    "\n",
    "        return loss, loss2, ap_acc, an_acc, triplets_used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_count = 70000 # put your number here\n",
    "cv_batches_count = 30000 # put your number here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_on_imporvemnet(file_path, sess, cv_loss, cv_losses):\n",
    "  #  save model when there is improvemnet in cv_loss value\n",
    "    if cv_losses == [] or cv_loss < np.min(cv_losses):\n",
    "        # save the entire model\n",
    "        saver = tf.train.Saver(max_to_keep = 1)\n",
    "        saver.save(sess, file_path)\n",
    "        print('Model saved')\n",
    "\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(file_path, epoch, train_loss, cv_loss, log_mode = 'a'):\n",
    "    # log train and cv losses\n",
    "    mode = log_mode if epoch == 0 else 'a'\n",
    "\n",
    "    with open(file_path, mode) as f:\n",
    "        if mode == 'w':\n",
    "            header = 'epoch, train_loss, cv_loss\\n'\n",
    "            f.write(header)\n",
    "\n",
    "        line = '%d, %f, %f\\n' %(epoch, train_loss, cv_loss)\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cool_down_mask(epoch):\n",
    "    if epoch > 0 and epoch % 20 == 0:\n",
    "        if config['masked_count'] >= 1:\n",
    "            config['masked_count'] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrease_learning_rate(epoch):\n",
    "    #if epoch > 0 and epoch % 10 == 0:\n",
    "    #    if config['learning_rate'] > 0:\n",
    "    #        config['learning_rate'] = config['learning_rate'] * (1 - 1e-5)\n",
    "    pass # this function does nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mask(size, num_ones):\n",
    "    mask = np.zeros(size, dtype = np.float32)\n",
    "    index = list(range(size))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    for idx in index[:num_ones]:\n",
    "        mask[idx] = 1.0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_placeholders_tensors():\n",
    "    # get model's placeholders and tensors\n",
    "    x = tf.placeholder(tf.float32, name = 'x', shape = [None,\n",
    "                                                        config['num_channels'],\n",
    "                                                        config['x_height'],\n",
    "                                                        config['x_width']])\n",
    "    dropout_rate = tf.placeholder(tf.float32, name = 'dropout_rate', shape = [2])\n",
    "    is_training = tf.placeholder(tf.bool, name = 'is_training')\n",
    "    triplets = tf.placeholder(tf.int32, name = 'triplets', shape = [None, 3])\n",
    "    same_threshold = tf.placeholder(tf.float32, name = 'same_threshold')\n",
    "    mask = tf.placeholder(tf.float32, name='mask')\n",
    "    \n",
    "\n",
    "    embeddings1, embeddings2 = build_model(x, dropout_rate, is_training, print_summary = True)\n",
    "\n",
    "    triplet_embedings1 = prepare_triplet_embeddings(triplets, embeddings1)\n",
    "    triplet_embedings2 = prepare_triplet_embeddings(triplets, embeddings2)\n",
    "    loss, loss2, ap_acc, an_acc, triplets_used  = triplet_loss_acc(triplet_embedings1, triplet_embedings2, mask, same_threshold)\n",
    "    optim = optimizer(loss)\n",
    "\n",
    "    placeholders_tensors = {'x': x,\n",
    "                            'dropout_rate': dropout_rate,\n",
    "                            'is_training': is_training,\n",
    "                            'triplets': triplets,\n",
    "                            'embeddings1': embeddings1,\n",
    "                            'embeddings2': embeddings2,\n",
    "                            'same_threshold': same_threshold,\n",
    "                            'mask': mask,\n",
    "                            'optimizer': optim,\n",
    "                            'loss': loss2,\n",
    "                            'ap_acc': ap_acc,\n",
    "                            'an_acc': an_acc,\n",
    "                            'triplets_used': triplets_used}\n",
    "    return placeholders_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, name = 'x', shape = [None,\n",
    "                                                        config['num_channels'],\n",
    "                                                        config['x_height'],\n",
    "                                                        config['x_width']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'x_2:0' shape=(?, 5, 50, 50) dtype=float32>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_per_batch(sess, batch, placeholders_tensors, epoch, threshold):\n",
    "    # train_per_batch and get train loss\n",
    "    tmp_loss, tmp_ap_acc, tmp_an_acc, tmp_triplets_used = [], [], [], []\n",
    "    t_total = 0\n",
    "    bad_batch_counter = 0\n",
    "\n",
    "    cool_down_mask(epoch)\n",
    "    decrease_learning_rate(epoch)\n",
    "    \n",
    "    for iteration in range(train_batches_count):\n",
    "        t_start = time.time()\n",
    "        print(\"Here\")\n",
    "        batch_x = sess.run(batch)\n",
    "        \n",
    "        batch_triplets = triplet_list_maker()\n",
    "        train_mask = build_mask(config['triplets_count'], config['masked_count'])\n",
    "        feed_dictionary = {placeholders_tensors['x']: batch_x,\n",
    "                           placeholders_tensors['dropout_rate']: config['dropout'],\n",
    "                           placeholders_tensors['is_training']: True,\n",
    "                           placeholders_tensors['triplets']: batch_triplets,\n",
    "                           placeholders_tensors['same_threshold']: threshold,\n",
    "                           placeholders_tensors['mask']: train_mask}\n",
    "        \n",
    "        \n",
    "\n",
    "        sess.run(placeholders_tensors['optimizer'], feed_dict = feed_dictionary)\n",
    "        train_loss = sess.run(placeholders_tensors['loss'], feed_dict = feed_dictionary)\n",
    "        train_ap_acc = sess.run(placeholders_tensors['ap_acc'], feed_dict = feed_dictionary)\n",
    "        train_an_acc = sess.run(placeholders_tensors['an_acc'], feed_dict = feed_dictionary)\n",
    "        train_triplets_used = sess.run(placeholders_tensors['triplets_used'], feed_dict = feed_dictionary)  \n",
    "\n",
    "        tmp_loss.append(train_loss)\n",
    "        tmp_ap_acc.append(train_ap_acc)\n",
    "        tmp_an_acc.append(train_an_acc)\n",
    "        tmp_triplets_used.append(train_triplets_used)\n",
    "\n",
    "        if int(train_triplets_used) != config ['triplets_count']:\n",
    "            bad_batch_counter += 1\n",
    "        t_total += (time.time() - t_start)\n",
    "\n",
    "        print(' '*200, end = '\\r')\n",
    "        print('epoch: %d, time: %f | train_loss: %f |' \\\n",
    "              ' ap_acc: %f | an_acc: %f | triplets_used: %d | bads: %d' %(epoch, t_total, train_loss,\n",
    "                                                               train_ap_acc, train_an_acc,\n",
    "                                                               int(train_triplets_used), bad_batch_counter),\n",
    "                                                               end = '\\r')\n",
    "    train_loss = np.mean(tmp_loss)\n",
    "    train_ap_acc = np.mean(tmp_ap_acc)\n",
    "    train_an_acc = np.mean(tmp_an_acc)\n",
    "    train_triplets_used = np.mean(tmp_triplets_used)\n",
    "    print(' '*200, end = '\\r')\n",
    "    print('epoch: %d, time: %f | train_loss: %f |' \\\n",
    "          ' ap_acc: %f | an_acc: %f | triplets_used: %d | bads: %d\\n' %(epoch, t_total, train_loss,\n",
    "                                                                       train_ap_acc, train_an_acc,\n",
    "                                                                       int(train_triplets_used), bad_batch_counter),\n",
    "                                                                       end = '\\r')\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_per_batch(sess, batch, placeholders_tensors, epoch, threshold):\n",
    "    # cross-validate per batch and get cv loss and accuracy\n",
    "    tmp_loss, tmp_ap_acc, tmp_an_acc, tmp_triplets_used = [], [], [], []\n",
    "    t_total = 0\n",
    "    bad_batch_counter = 0\n",
    "\n",
    "    for iteration in range(cv_batches_count):\n",
    "        t_start = time.time()\n",
    "        batch_x = sess.run(batch)\n",
    "        batch_triplets = triplet_list_maker()\n",
    "        test_mask = build_mask(config['triplets_count'], config['masked_count'])\n",
    "        cv_feed_dictionary = {placeholders_tensors['x']: batch_x,\n",
    "                              placeholders_tensors['dropout_rate']: [0.0, 0.0],\n",
    "                              placeholders_tensors['is_training']: False,\n",
    "                              placeholders_tensors['triplets']: batch_triplets,\n",
    "                              placeholders_tensors['same_threshold']: threshold,\n",
    "                              placeholders_tensors['mask']: test_mask}\n",
    "        \n",
    "        cv_loss = sess.run(placeholders_tensors['loss'], feed_dict = cv_feed_dictionary)\n",
    "        cv_ap_acc = sess.run(placeholders_tensors['ap_acc'], feed_dict = cv_feed_dictionary)\n",
    "        cv_an_acc = sess.run(placeholders_tensors['an_acc'], feed_dict = cv_feed_dictionary)\n",
    "        cv_triplets_used = sess.run(placeholders_tensors['triplets_used'], feed_dict = cv_feed_dictionary)\n",
    "\n",
    "        tmp_loss.append(cv_loss)\n",
    "        tmp_ap_acc.append(cv_ap_acc)\n",
    "        tmp_an_acc.append(cv_an_acc)\n",
    "        tmp_triplets_used.append(cv_triplets_used)\n",
    "\n",
    "        if int(cv_triplets_used) != config['triplets_count']:\n",
    "            bad_batch_counter += 1\n",
    "        t_total += (time.time() - t_start)\n",
    "\n",
    "        print(' '*200, end = '\\r')\n",
    "        print('time: %f | cv_loss: %f |' \\\n",
    "              ' cv_ap_acc: %f | cv_an_acc: %f | cv_triplets_used: %d | cv_bads: %d' %(t_total, cv_loss,\n",
    "                                                                                      cv_ap_acc, cv_an_acc,\n",
    "                                                                                      int(cv_triplets_used),\n",
    "                                                                                      bad_batch_counter), end = '\\r')\n",
    "    cv_loss = np.mean(tmp_loss)\n",
    "    cv_ap_acc = np.mean(tmp_ap_acc)\n",
    "    cv_an_acc = np.mean(tmp_an_acc)\n",
    "    cv_triplets_used = np.mean(tmp_triplets_used)\n",
    "    print(' '*200, end = '\\r')\n",
    "    print('time: %f | cv_loss: %f |' \\\n",
    "          ' cv_ap_acc: %f | cv_an_acc: %f | cV_triplets_used: %d | cV_bads: %d' %(t_total, cv_loss,\n",
    "                                                                                  cv_ap_acc, cv_an_acc,\n",
    "                                                                                  int(cv_triplets_used),\n",
    "                                                                                  bad_batch_counter))\n",
    "    return cv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, init_epoch, queue_size, num_threads, resume, threshold):\n",
    "    # train model\n",
    "    train_losses, cv_losses = [], []\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    placeholders_tensors = get_placeholders_tensors()\n",
    "    train_batch  = read_from_tfrecord([config['tfrecords_train']], queue_size, num_threads,\n",
    "                                      np.int(0.5 * queue_size))\n",
    "    cv_batch  = read_from_tfrecord([config['tfrecords_cv']], 50, 2, 10)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess = sess, coord = coord)\n",
    "\n",
    "        if resume:\n",
    "            print('loading weights....')\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, (config['model_path']))  # to load the best saved model so far replace\n",
    "                                                              # \"curr_model_path\" with \"model_path\"\n",
    "\n",
    "            # load loss and accuracy so that less accurate model\n",
    "            # won't be saved after resume\n",
    "            tmp = np.genfromtxt(config['log_path'], delimiter = ',', names = True)\n",
    "            train_losses = list(tmp['train_loss'])\n",
    "            cv_losses = list(tmp['cv_loss'])\n",
    "            del tmp\n",
    "        else:\n",
    "            print('initializing weights....')\n",
    "            init_op =tf.group(tf.local_variables_initializer(), tf.global_variables_initializer())\n",
    "            sess.run(init_op)\n",
    "        #sess.run(tf.local_variables_initializer())\n",
    "        print('training....')\n",
    "        for epoch in range(init_epoch, epochs):\n",
    "            # training\n",
    "            train_loss = train_per_batch(sess,train_batch, \n",
    "                                         placeholders_tensors, epoch, threshold)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            #cross-validation\n",
    "            cv_loss = cv_per_batch(sess, cv_batch, \n",
    "                                   placeholders_tensors, epoch, threshold)\n",
    "\n",
    "            #save model\n",
    "            save_model_on_imporvemnet(config['model_path'], sess, cv_loss, cv_losses)\n",
    "            cv_losses.append(cv_loss)\n",
    "\n",
    "            # log results\n",
    "            log_loss(config['log_path'], epoch, train_loss, cv_loss,\n",
    "                     log_mode = ('a' if resume else 'w'))\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        return train_losses, cv_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch=read_from_tfrecord([config['tfrecords_train']], 500, 10,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_tfrecord2(tfrecord_file, queue_size, num_threads, min_capacity):\n",
    "    # read batch from tfrecords file\n",
    "    with tf.variable_scope('Queue_Batch_Shuffle', reuse = False) as scope:\n",
    "        tfrecord_file_queue = tf.train.string_input_producer(tfrecord_file, name = 'queue')\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, tfrecord_serialized = reader.read(tfrecord_file_queue)\n",
    "        tfrecord_features = tf.parse_single_example(tfrecord_serialized,\n",
    "                    features = {'batch': tf.FixedLenFeature([], tf.string)},\n",
    "                    name = 'features')\n",
    "\n",
    "        batch_data = tf.decode_raw(tfrecord_features['batch'], tf.float32)\n",
    "        batch_data = tf.reshape(batch_data, [config['batch_size'],\n",
    "                                             config['num_channels'],\n",
    "                                             config['x_height'],\n",
    "                                             config['x_width']])\n",
    "        \n",
    "        print(batch_data)\n",
    "\n",
    "        batch_data_shuffled = tf.train.shuffle_batch([batch_data],\n",
    "                                                      batch_size = 1,\n",
    "                                                      capacity = queue_size,\n",
    "                                                      num_threads = num_threads,\n",
    "                                                      min_after_dequeue = min_capacity)\n",
    "        return batch_data_shuffled[0],batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./TF_Records_File_train.tfrecords'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['tfrecords_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Queue_Batch_Shuffle_4/Reshape:0\", shape=(256, 5, 50, 50), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Queue_Batch_Shuffle_4/strided_slice:0' shape=(256, 5, 50, 50) dtype=float32>,\n",
       " <tf.Tensor 'Queue_Batch_Shuffle_4/Reshape:0' shape=(256, 5, 50, 50) dtype=float32>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_from_tfrecord2([config['tfrecords_train']], 500, 10,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model summary:\n",
      " x: (?, 5, 50, 50)\n",
      " conv1: (?, 25, 50, 50)\n",
      " max_pool1: (?, 25, 25, 25)\n",
      " conv2: (?, 50, 25, 25)\n",
      " max_pool2: (?, 50, 12, 12)\n",
      " conv3: (?, 100, 12, 12)\n",
      " max_pool3: (?, 100, 6, 6)\n",
      " conv4: (?, 100, 6, 6)\n",
      " max_pool4 (?, 100, 3, 3)\n",
      " flatten5 (?, 900)\n",
      " fc5 (?, 1024)\n",
      " fc6 (?, 1024)\n",
      " fc7 (?, 256)\n",
      " fc8 (?, 256)\n",
      "\n",
      "initializing weights....\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, ./TF_Records_File_train.tfrecords; No such file or directory\n",
      "\t [[{{node Queue_Batch_Shuffle/ReaderReadV2}}]]\n",
      "training....\n",
      "Here\n"
     ]
    },
    {
     "ename": "OutOfRangeError",
     "evalue": "RandomShuffleQueue '_41_Queue_Batch_Shuffle/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)\n\t [[node Queue_Batch_Shuffle/shuffle_batch (defined at <ipython-input-14-d61be88a2a6e>:17) ]]\n\nOriginal stack trace for 'Queue_Batch_Shuffle/shuffle_batch':\n  File \"/opt/anaconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/anaconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/anaconda3/lib/python3.8/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/anaconda3/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/opt/anaconda3/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"/opt/anaconda3/lib/python3.8/asyncio/events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n    self.do_execute(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2866, in run_cell\n    result = self._run_cell(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3071, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-72-48faf35e074a>\", line 1, in <module>\n    train_model(epochs = 1000, init_epoch = 8,\n  File \"<ipython-input-71-4916905daf50>\", line 7, in train_model\n    train_batch  = read_from_tfrecord([config['tfrecords_train']], queue_size, num_threads,\n  File \"<ipython-input-14-d61be88a2a6e>\", line 17, in read_from_tfrecord\n    batch_data_shuffled = tf.train.shuffle_batch([batch_data],\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/input.py\", line 1335, in shuffle_batch\n    return _shuffle_batch(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/input.py\", line 874, in _shuffle_batch\n    dequeued = queue.dequeue_many(batch_size, name=name)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 487, in dequeue_many\n    ret = gen_data_flow_ops.queue_dequeue_many_v2(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 3566, in queue_dequeue_many_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 742, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3477, in _create_op_internal\n    ret = Operation(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1949, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1350\u001b[0m                                       target_list, run_metadata)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1440\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1441\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: RandomShuffleQueue '_41_Queue_Batch_Shuffle/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)\n\t [[{{node Queue_Batch_Shuffle/shuffle_batch}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-48faf35e074a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_model(epochs = 1000, init_epoch = 8,\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mqueue_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             resume = False, threshold = 0.84)\n",
      "\u001b[0;32m<ipython-input-71-4916905daf50>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epochs, init_epoch, queue_size, num_threads, resume, threshold)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             train_loss = train_per_batch(sess,train_batch, \n\u001b[0m\u001b[1;32m     36\u001b[0m                                          placeholders_tensors, epoch, threshold)\n\u001b[1;32m     37\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-8ff199449121>\u001b[0m in \u001b[0;36mtrain_per_batch\u001b[0;34m(sess, batch, placeholders_tensors, epoch, threshold)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mt_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Here\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mbatch_triplets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriplet_list_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    958\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1181\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1359\u001b[0m                            run_metadata)\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: RandomShuffleQueue '_41_Queue_Batch_Shuffle/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 1, current size 0)\n\t [[node Queue_Batch_Shuffle/shuffle_batch (defined at <ipython-input-14-d61be88a2a6e>:17) ]]\n\nOriginal stack trace for 'Queue_Batch_Shuffle/shuffle_batch':\n  File \"/opt/anaconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/anaconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/anaconda3/lib/python3.8/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/anaconda3/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/opt/anaconda3/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"/opt/anaconda3/lib/python3.8/asyncio/events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n    self.do_execute(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2866, in run_cell\n    result = self._run_cell(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3071, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-72-48faf35e074a>\", line 1, in <module>\n    train_model(epochs = 1000, init_epoch = 8,\n  File \"<ipython-input-71-4916905daf50>\", line 7, in train_model\n    train_batch  = read_from_tfrecord([config['tfrecords_train']], queue_size, num_threads,\n  File \"<ipython-input-14-d61be88a2a6e>\", line 17, in read_from_tfrecord\n    batch_data_shuffled = tf.train.shuffle_batch([batch_data],\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/input.py\", line 1335, in shuffle_batch\n    return _shuffle_batch(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/input.py\", line 874, in _shuffle_batch\n    dequeued = queue.dequeue_many(batch_size, name=name)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 487, in dequeue_many\n    ret = gen_data_flow_ops.queue_dequeue_many_v2(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 3566, in queue_dequeue_many_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 742, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3477, in _create_op_internal\n    ret = Operation(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1949, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "train_model(epochs = 1000, init_epoch = 8,\n",
    "            queue_size = 500, num_threads = 10,\n",
    "            resume = False, threshold = 0.84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
